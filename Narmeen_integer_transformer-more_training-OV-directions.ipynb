{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7SoLlhxcaczO",
    "outputId": "d13231e9-396f-4e00-db3f-6d6a62a2d9a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformer_lens in /opt/conda/lib/python3.10/site-packages (1.3.0)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /opt/conda/lib/python3.10/site-packages (from transformer_lens) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/conda/lib/python3.10/site-packages (from transformer_lens) (1.24.3)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /opt/conda/lib/python3.10/site-packages (from transformer_lens) (2.0.3)\n",
      "Requirement already satisfied: datasets>=2.7.1 in /opt/conda/lib/python3.10/site-packages (from transformer_lens) (2.13.1)\n",
      "Requirement already satisfied: einops>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from transformer_lens) (0.6.1)\n",
      "Requirement already satisfied: typeguard<4.0.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from transformer_lens) (3.0.2)\n",
      "Requirement already satisfied: rich>=12.6.0 in /opt/conda/lib/python3.10/site-packages (from transformer_lens) (13.4.2)\n",
      "Requirement already satisfied: wandb>=0.13.5 in /opt/conda/lib/python3.10/site-packages (from transformer_lens) (0.15.5)\n",
      "Requirement already satisfied: jaxtyping>=0.2.11 in /opt/conda/lib/python3.10/site-packages (from transformer_lens) (0.2.20)\n",
      "Requirement already satisfied: torch>=1.10 in /opt/conda/lib/python3.10/site-packages (from transformer_lens) (1.13.1)\n",
      "Requirement already satisfied: transformers>=4.25.1 in /opt/conda/lib/python3.10/site-packages (from transformer_lens) (4.30.2)\n",
      "Requirement already satisfied: fancy-einsum>=0.0.3 in /opt/conda/lib/python3.10/site-packages (from transformer_lens) (0.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (2.29.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (3.8.4)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (2023.6.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (0.3.6)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (3.2.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (0.16.4)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (0.70.14)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (6.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (12.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.1 in /opt/conda/lib/python3.10/site-packages (from jaxtyping>=0.2.11->transformer_lens) (4.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.5->transformer_lens) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.5->transformer_lens) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.5->transformer_lens) (2023.3)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=12.6.0->transformer_lens) (2.15.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=12.6.0->transformer_lens) (3.0.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10->transformer_lens) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10->transformer_lens) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10->transformer_lens) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10->transformer_lens) (11.7.99)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10->transformer_lens) (0.38.4)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10->transformer_lens) (65.6.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.25.1->transformer_lens) (0.3.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers>=4.25.1->transformer_lens) (3.9.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.25.1->transformer_lens) (0.13.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.25.1->transformer_lens) (2023.6.3)\n",
      "Requirement already satisfied: pathtools in /opt/conda/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (1.3.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (3.20.3)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (1.28.1)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (1.4.4)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (8.1.5)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (5.9.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (0.4.0)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (3.1.32)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.16.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (2.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.9.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (23.1.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.10)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (3.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (0.6.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: plotly in /opt/conda/lib/python3.10/site-packages (5.15.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly) (8.2.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from plotly) (23.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: jaxtyping in /opt/conda/lib/python3.10/site-packages (0.2.20)\n",
      "Requirement already satisfied: typeguard>=2.13.3 in /opt/conda/lib/python3.10/site-packages (from jaxtyping) (3.0.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /opt/conda/lib/python3.10/site-packages (from jaxtyping) (1.24.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.1 in /opt/conda/lib/python3.10/site-packages (from jaxtyping) (4.5.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: protobuf==3.20.* in /opt/conda/lib/python3.10/site-packages (3.20.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pytorch_lightning in /opt/conda/lib/python3.10/site-packages (2.0.5)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (4.5.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (1.13.1)\n",
      "Requirement already satisfied: lightning-utilities>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (0.9.0)\n",
      "Requirement already satisfied: packaging>=17.1 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (23.0)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (4.65.0)\n",
      "Requirement already satisfied: fsspec[http]>2021.06.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (2023.6.0)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (1.24.3)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (1.0.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (2.29.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (3.8.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->pytorch_lightning) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->pytorch_lightning) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->pytorch_lightning) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->pytorch_lightning) (8.5.0.96)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.11.0->pytorch_lightning) (0.38.4)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.11.0->pytorch_lightning) (65.6.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (23.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.9.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (2.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2023.5.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting pytorch\n",
      "  Using cached pytorch-1.0.2.tar.gz (689 bytes)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: pytorch\n",
      "  Building wheel for pytorch (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[6 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-07dq4sj3/pytorch_071dab85d0b54f6093b66d7e56843a90/setup.py\", line 15, in <module>\n",
      "  \u001b[31m   \u001b[0m     raise Exception(message)\n",
      "  \u001b[31m   \u001b[0m Exception: You tried to install \"pytorch\". The package named for PyTorch is \"torch\"\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m  ERROR: Failed building wheel for pytorch\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for pytorch\n",
      "Failed to build pytorch\n",
      "Installing collected packages: pytorch\n",
      "  Running setup.py install for pytorch ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mRunning setup.py install for pytorch\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[6 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-07dq4sj3/pytorch_071dab85d0b54f6093b66d7e56843a90/setup.py\", line 11, in <module>\n",
      "  \u001b[31m   \u001b[0m     raise Exception(message)\n",
      "  \u001b[31m   \u001b[0m Exception: You tried to install \"pytorch\". The package named for PyTorch is \"torch\"\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mlegacy-install-failure\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while trying to install package.\n",
      "\u001b[31m╰─>\u001b[0m pytorch\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for output from the failure.\n",
      "\u001b[?25hNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.24.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.65.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.7.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (4.41.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.24.3)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: circuitsvis in /opt/conda/lib/python3.10/site-packages (1.40.0)\n",
      "Requirement already satisfied: importlib-metadata<6.0.0,>=5.1.0 in /opt/conda/lib/python3.10/site-packages (from circuitsvis) (5.2.0)\n",
      "Requirement already satisfied: torch>=1.10 in /opt/conda/lib/python3.10/site-packages (from circuitsvis) (1.13.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.23 in /opt/conda/lib/python3.10/site-packages (from circuitsvis) (1.24.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<6.0.0,>=5.1.0->circuitsvis) (3.16.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10->circuitsvis) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10->circuitsvis) (11.7.99)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10->circuitsvis) (4.5.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10->circuitsvis) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10->circuitsvis) (11.10.3.66)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10->circuitsvis) (0.38.4)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10->circuitsvis) (65.6.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformer_lens\n",
    "%pip install einops\n",
    "%pip install plotly\n",
    "%pip install jaxtyping\n",
    "%pip install protobuf==3.20.*\n",
    "%pip install pytorch_lightning\n",
    "%pip install pytorch\n",
    "%pip install numpy\n",
    "%pip install tqdm\n",
    "%pip install matplotlib\n",
    "%pip install circuitsvis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cPGVe_j2a1_5"
   },
   "outputs": [],
   "source": [
    "#import statements\n",
    "import torch \n",
    "import numpy as np\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, clear_output, Image as IPyImage\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "import functools\n",
    "from transformer_lens import utils\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import plotly.io as pio\n",
    "import circuitsvis as cv\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "pio.renderers.default = \"png\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nice Normal Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-0eS0oFfboFF"
   },
   "outputs": [],
   "source": [
    "def generate_digit_dataset(n_examples, n_digits, reversed_output=False, n_equals=0):\n",
    "    # Generate a dataset of equations that involve an addition between two numbers with n_digits\n",
    "    assert n_examples <= 10 ** (n_digits * 2)\n",
    "    dataset = set()\n",
    "    while len(dataset) != n_examples:\n",
    "        # Sample random numbers\n",
    "        a = np.random.randint(0, 10**n_digits)\n",
    "        b = np.random.randint(0, 10**n_digits)\n",
    "        dataset.add(generate_digit_datapoint(a, b, n_digits, reversed_output, n_equals))\n",
    "    return list(dataset)\n",
    "\n",
    "def generate_digit_datapoint(a, b, n_digits, reversed_output, n_equals):\n",
    "    bos = ''\n",
    "    sep = '=' * n_equals\n",
    "    a_fwd = f'{a:>0{n_digits}d}'\n",
    "    b_fwd = f'{b:>0{n_digits}d}'\n",
    "    c_fwd = f'{a+b:>0{n_digits+1}d}'\n",
    "    if reversed_output:\n",
    "        return f\"{bos}{a_fwd[::-1]}{b_fwd[::-1]}{sep}{c_fwd[::-1]}\"\n",
    "    else:\n",
    "        return f\"{bos}{a_fwd}{b_fwd}{sep}{c_fwd}\"\n",
    "\n",
    "def tokenize_strings(strings):\n",
    "    vocab = \"0123456789=^\"\n",
    "    vocab2idx = {v: i for i, v in enumerate(vocab)}\n",
    "    # Tokenize a list of strings\n",
    "    tokens = np.zeros((len(strings), len(strings[0])), dtype=np.int64)\n",
    "    for i, string in enumerate(strings):\n",
    "        idxs = [vocab2idx[char] for char in list(string)]\n",
    "        tokens[i] = np.array(idxs)\n",
    "    return tokens\n",
    "\n",
    "class TokenizedDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, tokens):\n",
    "        # Tokens should be of shape N_examples x N_chars x D_tokens\n",
    "        self.tokens = torch.tensor(tokens).long()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Should return an example of N_chars x D_tokens\n",
    "        return self.tokens[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tokens.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "shAGHNsue49V"
   },
   "outputs": [],
   "source": [
    "def construct_model(n_layers, n_heads, d_head, attn_only=False, disable_b=False):\n",
    "    # Create a HookedTransformer\n",
    "    print(f'construct_model n_heads={n_heads}, d_head={d_head}')\n",
    "    cfg = HookedTransformerConfig(\n",
    "        n_layers=n_layers,\n",
    "        n_heads=n_heads,\n",
    "        d_model=128,\n",
    "        d_head=d_head,\n",
    "        d_mlp=512 if not attn_only else None,\n",
    "        act_fn=\"gelu\" if not attn_only else None,\n",
    "        attn_only=attn_only,\n",
    "        normalization_type=None,\n",
    "        attention_dir=\"causal\",\n",
    "        d_vocab=12,\n",
    "        d_vocab_out=10,\n",
    "        n_ctx=3*n_digits+1+n_equals, # Create appropriate context size for strings\n",
    "        init_weights=True,\n",
    "        device=\"cuda\"\n",
    "    )\n",
    "    model = HookedTransformer(cfg)\n",
    "    if disable_b:\n",
    "        for name, param in model.named_parameters():\n",
    "            if \"b_\" in name:\n",
    "                param.requires_grad = False\n",
    "    return model\n",
    "\n",
    "def loss_fn(logits, labels):\n",
    "    # Logits is shape B x P x D, labels is B x P\n",
    "    log_probs = logits.reshape(-1, logits.shape[-1]).log_softmax(dim=-1)\n",
    "    labels = labels.reshape(-1)\n",
    "    correct_log_probs = log_probs.gather(dim=-1, index=labels[:, None])[:, 0]\n",
    "    return -correct_log_probs.mean()\n",
    "\n",
    "def acc_fn(logits, labels):\n",
    "    with torch.no_grad():\n",
    "        # We can't differentiate this\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        correct = preds == labels\n",
    "        return torch.sum(correct).item() / np.prod(correct.shape)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, test_loader, n_epochs, verbose=False,):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, ) #betas=(0.9, 0.98), weight_decay=1.,)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, n_epochs)\n",
    "\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    \n",
    "    epoch_tqdm = tqdm(total=n_epochs, desc='Training model', leave=True)\n",
    "    loss_plot = widgets.Output()\n",
    "\n",
    "    # Display widgets\n",
    "    display(epoch_tqdm)\n",
    "    display(loss_plot)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_tqdm.update(1) # update progress bar\n",
    "\n",
    "        acc = []\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            n_batch, n_pos = batch.shape\n",
    "            cutoff = n_pos - n_digits - 1\n",
    "            X = torch.cat((batch[:, :cutoff], 10 * torch.ones(n_batch, n_digits + 1, dtype=torch.long)), dim=1).cuda() # Pad with = where the output should be\n",
    "            labels = batch[:, cutoff:].cuda()\n",
    "            logits = model(X)[:, cutoff:]\n",
    "            loss = loss_fn(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            acc.append(acc_fn(logits, labels))\n",
    "        train_acc.append(np.array(acc).mean())\n",
    "\n",
    "\n",
    "        acc = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                n_batch, n_pos = batch.shape\n",
    "                cutoff = n_pos - n_digits - 1\n",
    "                X = torch.cat((batch[:, :cutoff], 10 * torch.ones(n_batch, n_digits + 1, dtype=torch.long)), dim=1).cuda()\n",
    "                labels = batch[:, cutoff:].cuda()\n",
    "                logits = model(X)[:, cutoff:]\n",
    "                acc.append(acc_fn(logits, labels))\n",
    "        test_acc.append(np.array(acc).mean())\n",
    "\n",
    "        scheduler.step()\n",
    "        if verbose:\n",
    "            print(f\"Epoch: {epoch + 1}, Train Acc: {train_acc[-1]*100:.3f}%, Test Acc: {test_acc[-1]*100:.3f}%\")\n",
    "        with loss_plot:\n",
    "            plt.clf() # clear current figure\n",
    "            plt.plot(train_acc, label='Train acc')\n",
    "            plt.plot(test_acc, label='Test acc')\n",
    "            plt.title('Accuracy over Time')\n",
    "            plt.xlabel('Iteration')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.legend()\n",
    "            clear_output(wait=True)\n",
    "            display(plt.gcf()) # display current figure\n",
    "            plt.close() # close current figure to free memory\n",
    "\n",
    "    return train_acc, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evil interactivity functions that use globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_and_train(\n",
    "    _n_digits,\n",
    "    _reversed_output,\n",
    "    _n_equals,\n",
    "    _n_heads,\n",
    "    _d_head,\n",
    "    _n_train_set,\n",
    "    _n_test_set,\n",
    "    _batch_size,\n",
    "    _n_epochs\n",
    "):\n",
    "    global n_digits, reversed_output, n_equals, n_train_set, n_test_set, batch_size, n_epochs, n_heads, d_head\n",
    "    global strings, tokens, train_set, test_set\n",
    "    global train_loader, test_loader\n",
    "    global model\n",
    "    global train_acc, test_acc\n",
    "    \n",
    "    n_digits        = _n_digits\n",
    "    reversed_output = _reversed_output\n",
    "    n_equals        = _n_equals\n",
    "    n_heads         = _n_heads\n",
    "    d_head          = _d_head\n",
    "    n_train_set     = _n_train_set\n",
    "    n_test_set      = _n_test_set\n",
    "    batch_size      = _batch_size\n",
    "    n_epochs        = _n_epochs\n",
    "\n",
    "    strings   = generate_digit_dataset(n_train_set + n_test_set, n_digits, reversed_output, n_equals)\n",
    "    tokens    = tokenize_strings(strings)\n",
    "    train_set = TokenizedDataset(tokens[:n_train_set])\n",
    "    test_set  = TokenizedDataset(tokens[n_train_set:])\n",
    "\n",
    "    # Create Dataloaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    test_loader  = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "    \n",
    "    # And then actually train the model\n",
    "    model = construct_model(1, n_heads=n_heads, d_head=d_head, attn_only=True)\n",
    "    train_acc, test_acc = train_model(model, train_loader, test_loader, n_epochs, verbose=False)\n",
    "\n",
    "    print(f\"Train acc: {train_acc[-1]}\")\n",
    "    print(f\"Test acc: {test_acc[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_attn_pattern(a=1234, b=1234):\n",
    "    global model, n_digits, reversed_output, n_equals\n",
    "    \n",
    "    input_str = generate_digit_datapoint(a, b, n_digits, reversed_output, n_equals)\n",
    "    tokens = tokenize_strings([input_str,])\n",
    "\n",
    "    _,size = tokens.shape\n",
    "    string_tokens = []\n",
    "    for i in range(size):\n",
    "      string_tokens.append(str(tokens[0][i]))\n",
    "    print(string_tokens)\n",
    "    tokens = torch.tensor(tokens).long().cuda()\n",
    "    output = model(tokens).argmax(2)\n",
    "\n",
    "    print(\"Here is the output\",output[0, -(n_digits+1):])\n",
    "\n",
    "    logits, cache = model.run_with_cache(tokens)\n",
    "    html = cv.attention.attention_patterns(\n",
    "        tokens = string_tokens,\n",
    "        attention=cache[\"pattern\", 0][0]\n",
    "    )\n",
    "\n",
    "    return display(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "construct_model n_heads=32, d_head=2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0098dfcb645445bebf492c402f8bc42a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training model:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tqdm.notebook.tqdm_notebook at 0x7f3c50c63e20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a94f629a865046e596b7aa180a7cc8e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m n_epochs        \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Configure the training run the first time, so that just running the notebook end\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# to end works\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mconfigure_and_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_digits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreversed_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_equals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_head\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_train_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_test_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 39\u001b[0m, in \u001b[0;36mconfigure_and_train\u001b[0;34m(_n_digits, _reversed_output, _n_equals, _n_heads, _d_head, _n_train_set, _n_test_set, _batch_size, _n_epochs)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# And then actually train the model\u001b[39;00m\n\u001b[1;32m     38\u001b[0m model \u001b[38;5;241m=\u001b[39m construct_model(\u001b[38;5;241m1\u001b[39m, n_heads\u001b[38;5;241m=\u001b[39mn_heads, d_head\u001b[38;5;241m=\u001b[39md_head, attn_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 39\u001b[0m train_acc, test_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 70\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, test_loader, n_epochs, verbose)\u001b[0m\n\u001b[1;32m     68\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(logits, labels)\n\u001b[1;32m     69\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 70\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     acc\u001b[38;5;241m.\u001b[39mappend(acc_fn(logits, labels))\n\u001b[1;32m     72\u001b[0m train_acc\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39marray(acc)\u001b[38;5;241m.\u001b[39mmean())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:68\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     67\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adamw.py:162\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    158\u001b[0m             max_exp_avg_sqs\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    160\u001b[0m         state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 162\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m          \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m          \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m          \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m          \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m          \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m          \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m          \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m          \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m          \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m          \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m          \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adamw.py:219\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 219\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adamw.py:270\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    267\u001b[0m step_t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# Perform stepweight decay\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    273\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set the defaults\n",
    "n_digits        = 6\n",
    "reversed_output = True\n",
    "n_equals        = 0\n",
    "n_heads         = 32\n",
    "d_head          = 2\n",
    "n_train_set     = 25_000\n",
    "n_test_set      = 2_000\n",
    "batch_size      = 256\n",
    "n_epochs        = 1000\n",
    "\n",
    "# Configure the training run the first time, so that just running the notebook end\n",
    "# to end works\n",
    "configure_and_train(\n",
    "    n_digits,\n",
    "    reversed_output,\n",
    "    n_equals,\n",
    "    n_heads,\n",
    "    d_head,\n",
    "    n_train_set,\n",
    "    n_test_set,\n",
    "    batch_size,\n",
    "    n_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# And then allow the user to configure it for later runs\n",
    "interact_manual.options(manual_name=\"Retrain model\")(\n",
    "    configure_and_train,\n",
    "    _n_digits        = widgets.BoundedIntText(value=n_digits, min=1, max=12),\n",
    "    _reversed_output = widgets.Checkbox(value=reversed_output),\n",
    "    _n_equals        = widgets.BoundedIntText(value=n_equals, min=0, max=3),\n",
    "    _n_heads         = widgets.BoundedIntText(value=n_heads, min=1, max=32),\n",
    "    _d_head          = widgets.BoundedIntText(value=d_head, min=1, max=128),\n",
    "    _n_train_set     = widgets.BoundedIntText(value=n_train_set, min=1, max=1_000_000),\n",
    "    _n_test_set      = widgets.BoundedIntText(value=n_test_set, min=1, max=1_000_000),\n",
    "    _batch_size      = widgets.BoundedIntText(value=batch_size, min=1, max=65_536),\n",
    "    _n_epochs        = widgets.BoundedIntText(value=n_epochs, min=1, max=10_000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "interact_manual(\n",
    "    view_attn_pattern,\n",
    "    a=widgets.BoundedIntText(value=1234, min=0, max=999999),\n",
    "    b=widgets.BoundedIntText(value=1234, min=0, max=999999),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "id": "SAac7f4RrUku",
    "outputId": "c90ea114-02e0-41e6-b437-eb8993bb26ed"
   },
   "source": [
    "# Logit Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hook_function(pattern, hook, head_index_not_to_ablate, n_heads=4):\n",
    "    for k in range(n_heads):\n",
    "        if (k != head_index_not_to_ablate):\n",
    "            pattern[:, :, k, :] = 0.0\n",
    "    return pattern\n",
    "\n",
    "def hook_function_zero_ablate_all_heads(pattern, hook, n_heads):\n",
    "    for k in range(n_heads):\n",
    "        pattern[:, :, k, :] = 0.0\n",
    "    return pattern\n",
    "\n",
    "def get_ablation_score_without_attn(model, tokens):\n",
    "    model.reset_hooks()\n",
    "    return model.run_with_hooks(tokens, fwd_hooks=[\n",
    "        (utils.get_act_name('v', 0), functools.partial(\n",
    "            hook_function_zero_ablate_all_heads,\n",
    "            n_heads=model.cfg.n_heads\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "def get_ablation_scores(\n",
    "    model,\n",
    "    tokens\n",
    "):\n",
    "    '''\n",
    "    Returns a tensor of shape (n_layers, n_heads) containing the increase in cross entropy loss from ablating the output of each head.\n",
    "    '''\n",
    "    ablation_scores = []\n",
    "\n",
    "    # Calculating loss without any ablation, to act as a baseline\n",
    "    model.reset_hooks()\n",
    "    #logits = model(tokens)\n",
    "    #print(\"Here are the actual logits\")\n",
    "    #print(logits[0][-(n_digits+ 1):].argmax(-1))\n",
    "\n",
    "    for head in range(model.cfg.n_heads):\n",
    "            # Use functools.partial to create a temporary hook function with the head number fixed\n",
    "        temp_hook_fn = functools.partial(hook_function, head_index_not_to_ablate=head, n_heads=model.cfg.n_heads)\n",
    "            # Run the model with the ablation hook\n",
    "        ablated_logits = model.run_with_hooks(tokens, fwd_hooks=[\n",
    "            (utils.get_act_name(\"v\",0), temp_hook_fn)\n",
    "        ])\n",
    "            \n",
    "        #return the ablated logits for each score\n",
    "        ablation_scores.append(ablated_logits)\n",
    "\n",
    "    return ablation_scores\n",
    "\n",
    "\n",
    "#for consecutive pairs of numbers (01), (12) ... (89), we see which heads boost up the likelihood of the correct logit\n",
    "list_tokens= []\n",
    "list_output= []\n",
    "list_correct_logits = []\n",
    "\n",
    "for i in range(9):\n",
    "#    input_str = f\"{str(i).zfill(n_digits)}{str(i+1).zfill(n_digits)}{(n_equals+n_digits+1) * '='}\"\n",
    "    input_str = generate_digit_datapoint(i, i+1, n_digits, reversed_output, n_equals)\n",
    "    tokens = tokenize_strings([input_str,])\n",
    "    _,size = tokens.shape\n",
    "    tokens = torch.tensor(tokens).long().cuda()\n",
    "    # the correct 2 digits logits\n",
    "    logits = model(tokens)\n",
    "    correct_logits = ((logits[0][-(2):]))\n",
    "    #print(\"correct logits\")\n",
    "    #print(correct_logits)\n",
    "    \n",
    "    list_tokens.append((str(i),str(i+1)))\n",
    "    ablation_scores = get_ablation_scores(model, tokens)\n",
    "    row = []\n",
    "    row_only_last_digit = []\n",
    "    for j in range(model.cfg.n_heads):\n",
    "        row.append(ablation_scores[j][0][-(n_digits + 1):].argmax(-1))\n",
    "        row_only_last_digit.append(ablation_scores[j][0][-7:][0])\n",
    "    list_output.append(row)\n",
    "    list_correct_logits.append(row_only_last_digit)\n",
    "    \n",
    "\n",
    "# we set up 9 tasks predicting the logits of the correct output corresponding to adding 9 consecutive numbers\n",
    "# 01, 12, 23, 34, 45, 56, 67, 78, 89\n",
    "# we look at which heads have contributed positively to the correct logit\n",
    "\n",
    "    \n",
    "#how much logit each head add to the correct output: We add 0 1, and we see how much each head add to the logit corresponding to 1    \n",
    "for k in range(9):\n",
    "    print(\"we are adding {} and {} and the correct last digit output is {}\".format(k, k+1,(2*k + 1)%10))\n",
    "    print(\"we print the logit contribution of each head to the correct output\")\n",
    "    for i in range(4):\n",
    "        print(\"logit of head\", i)\n",
    "        print(list_correct_logits[k][i][(2*k + 1)%10])\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_digit_additions_tokens = torch.tensor(tokenize_strings([\n",
    "    generate_digit_datapoint(a, b, n_digits, reversed_output, n_equals)\n",
    "    for a in range(10)\n",
    "    for b in range(10)\n",
    "])).long().cuda()\n",
    "\n",
    "ablation_scores = get_ablation_scores(\n",
    "    model,\n",
    "    one_digit_additions_tokens\n",
    ")\n",
    "print(ablation_scores[0].shape) # (batch, d_position, d_vocab)\n",
    "torch.stack(ablation_scores) # (head, batch, d_position, d_vocab)\n",
    "\n",
    "batch_index     = 0\n",
    "#head_index      = 0\n",
    "expected_output = 1\n",
    "probs = F.softmax(torch.stack(ablation_scores), dim=3) # (head, batch, position, d_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_dim, b_dim = 10, 10 # a and b are digits\n",
    "# And now we want a heat map of, for a single head,\n",
    "# a by b with color corresponding to how strongly\n",
    "# that head predicted the correct output\n",
    "head_probs  = F.softmax(torch.stack(ablation_scores), dim=3).cpu().detach().numpy()\n",
    "head_probs  = torch.stack(ablation_scores).cpu().detach().numpy()\n",
    "nice_scores = head_probs.reshape((model.cfg.n_heads, a_dim, b_dim, model.cfg.n_ctx, 10))\n",
    "#probabilities_subset = nice_scores[head_idx, :, :, position_idx, :]\n",
    "probs_by_head = np.zeros((model.cfg.n_heads, 10, 10)) # (head, a, b) -> correct probs\n",
    "position_idx = 2*n_digits -1\n",
    "for head_idx in range(model.cfg.n_heads):\n",
    "    for a in range(10):\n",
    "        for b in range(10):\n",
    "            c = (a+b)%10\n",
    "            probs_by_head[head_idx,a,b] = nice_scores[head_idx,a,b,position_idx,c]\n",
    "images = []\n",
    "image_path_template = \"heatmap_images_heatmap_{}.png\"\n",
    "for head_idx in range(model.cfg.n_heads):\n",
    "    plt.imshow(probs_by_head[head_idx], cmap='RdBu')#, vmin=-256, vmax=+256)\n",
    "    plt.colorbar()\n",
    "    plt.title(f\"Logits for correct digit output, only head={head_idx}\")\n",
    "    plt.savefig(image_path_template.format(head_idx))\n",
    "    plt.close()\n",
    "    image_path = image_path_template.format(head_idx)\n",
    "    image = Image.open(image_path)\n",
    "    images.append(image)\n",
    "\n",
    "gif_path = \"heatmap_animation.gif\"\n",
    "images[0].save(gif_path, save_all=True, append_images=images[1:], optimize=False, duration=200, loop=0)\n",
    "display(IPyImage(filename=gif_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ablation_score_no_attn = get_ablation_score_without_attn(model, one_digit_additions_tokens)\n",
    "a_dim, b_dim = 10, 10 # a and b are digits\n",
    "\n",
    "# With all heads ablated, what are the logits for the correct\n",
    "# digit for every pair in 1-digit addition\n",
    "nice_scores = ablation_score_no_attn.reshape((a_dim, b_dim, model.cfg.n_ctx, 10))\n",
    "probs = np.zeros((10, 10)) # (a, b) -> correct probs\n",
    "for a in range(10):\n",
    "    for b in range(10):\n",
    "        c = (a+b)%10\n",
    "        probs[a,b] = nice_scores[a,b,position_idx,c]\n",
    "probs.shape\n",
    "\n",
    "plt.imshow(probs, cmap='RdBu', vmin=-5, vmax=+5)\n",
    "plt.colorbar()\n",
    "plt.title(f\"Logits for correct digit output, all heads ablated\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation_score_no_attn = get_ablation_score_without_attn(model, one_digit_additions_tokens)\n",
    "a_dim, b_dim = 10, 10 # a and b are digits\n",
    "nice_scores = ablation_score_no_attn.reshape((a_dim, b_dim, model.cfg.n_ctx, 10))\n",
    "\n",
    "# Create a figure and axes with the desired layout\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
    "\n",
    "# Iterate over the charts and plot them in the corresponding axes\n",
    "for digit, ax in enumerate(axes.flatten()):\n",
    "    #  # Customize this line with your specific chart data and plot settings\n",
    "    #ax.set_title(f'Chart {digit+1}')\n",
    "    logits = np.zeros((10, 10)) # (a, b) -> logit_for_digit\n",
    "    for a in range(10):\n",
    "        for b in range(10):\n",
    "            logits[a,b] = nice_scores[a,b,position_idx,digit]\n",
    "    im = ax.imshow(logits, cmap='RdBu', vmin=-10, vmax=+10)\n",
    "    ax.set_title(f\"Logits for digit {digit}\\nAll heads ablated\")\n",
    "    ax.axis('off')  # Optional: Turn off axis labels and ticks\n",
    "    cax = fig.add_axes([ax.get_position().x1 + 0.01, ax.get_position().y0, 0.02, ax.get_position().height])\n",
    "    fig.colorbar(im, cax=cax)\n",
    "\n",
    "plt.tight_layout(pad=4.0)\n",
    "for digit in range(10):\n",
    "    digit_chart_pos = fig.axes[-20+digit].get_position()\n",
    "    dx = 0.001 if digit < 9 else 0.003333333\n",
    "    w  = 0.02  if digit < 9 else 0.014444444\n",
    "    fig.axes[-10+digit].set_position(dict(\n",
    "        x0=digit_chart_pos.x1+dx,\n",
    "        y0=digit_chart_pos.y0,\n",
    "        width=w,\n",
    "        height=digit_chart_pos.height\n",
    "    ).values())\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fig.axes[-6].get_position())\n",
    "print(fig.axes[-1].get_position())\n",
    "digit = 9\n",
    "digit_chart_pos = fig.axes[-20+digit].get_position()\n",
    "fig.axes[-10+digit].set_position(dict(\n",
    "    x0=digit_chart_pos.x1+0.00213888,\n",
    "    y0=digit_chart_pos.y0,\n",
    "    width=0.02*0.7222222,\n",
    "    height=digit_chart_pos.height\n",
    ").values())\n",
    "print(fig.axes[-1].get_position())\n",
    "0.9574814814814812-0.9558425917037032+0.00044444444"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_dim, b_dim = 10, 10 # a and b are digits\n",
    "# And now we want a heat map of, for a single head,\n",
    "# a by b with color corresponding to how strongly\n",
    "# that head predicted the correct output\n",
    "\n",
    "head_logits  = torch.stack(ablation_scores).cpu().detach().numpy()\n",
    "nice_scores = head_logits.reshape((model.cfg.n_heads, a_dim, b_dim, model.cfg.n_ctx, 10))\n",
    "probabilities_subset = nice_scores[head_idx, :, :, position_idx, :]\n",
    "probs_by_head = np.zeros((model.cfg.n_heads, 10, 10)) # (head, a, b) -> correct logits\n",
    "for head_idx in range(model.cfg.n_heads):\n",
    "    for a in range(10):\n",
    "        for b in range(10):\n",
    "            c = (a+b)%10\n",
    "            probs_by_head[head_idx,a,b] = nice_scores[head_idx,a,b,position_idx,c]\n",
    "images = []\n",
    "image_path_template = \"heatmap_multi_frame_{}.png\"\n",
    "for head_idx in range(model.cfg.n_heads):\n",
    "    # Create a figure and axes with the desired layout\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
    "\n",
    "    # Iterate over the charts and plot them in the corresponding axes\n",
    "    for digit, ax in enumerate(axes.flatten()):\n",
    "        #  # Customize this line with your specific chart data and plot settings\n",
    "        #ax.set_title(f'Chart {digit+1}')\n",
    "        logits = np.zeros((10, 10)) # (a, b) -> logit_for_digit\n",
    "        for a in range(10):\n",
    "            for b in range(10):\n",
    "                logits[a,b] = nice_scores[head_idx,a,b,position_idx,digit]\n",
    "        im = ax.imshow(logits, cmap='RdBu', vmin=-256, vmax=+256)\n",
    "        ax.set_title(f\"Logits for digit {digit}\\nOnly head={head_idx}\")\n",
    "        #ax.axis('off')  # Optional: Turn off axis labels and ticks\n",
    "        cax = fig.add_axes([ax.get_position().x1 + 0.01, ax.get_position().y0, 0.02, ax.get_position().height])\n",
    "        fig.colorbar(im, cax=cax)\n",
    "\n",
    "    plt.tight_layout(pad=4.0)\n",
    "    for digit in range(10):\n",
    "        digit_chart_pos = fig.axes[-20+digit].get_position()\n",
    "        dx = 0.001 if digit < 9 else 0.003333333\n",
    "        w  = 0.02  if digit < 9 else 0.014444444\n",
    "        fig.axes[-10+digit].set_position(dict(\n",
    "            x0=digit_chart_pos.x1+dx,\n",
    "            y0=digit_chart_pos.y0,\n",
    "            width=w,\n",
    "            height=digit_chart_pos.height\n",
    "        ).values())\n",
    "    plt.savefig(image_path_template.format(head_idx))\n",
    "    plt.close()\n",
    "\n",
    "    image_path = image_path_template.format(head_idx)\n",
    "    image = Image.open(image_path)\n",
    "    images.append(image)\n",
    "\n",
    "gif_path = \"heatmap_multi_animation.gif\"\n",
    "images[0].save(gif_path, save_all=True, append_images=images[1:], optimize=False, duration=200, loop=0)\n",
    "display(IPyImage(filename=gif_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, value in model.named_parameters():\n",
    "    print(f\"{name}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default = \"png\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See how the embed + pos embed projects into attention space for each head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all the layers of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_named_params = dict(model.named_parameters())\n",
    "W_E   = model_named_params['embed.W_E']\n",
    "W_pos = model_named_params['pos_embed.W_pos']\n",
    "\n",
    "W_Q   = model_named_params['blocks.0.attn.W_Q']\n",
    "W_K   = model_named_params['blocks.0.attn.W_K']\n",
    "W_O   = model_named_params['blocks.0.attn.W_O']\n",
    "W_V   = model_named_params['blocks.0.attn.W_V']\n",
    "\n",
    "b_Q   = model_named_params['blocks.0.attn.b_Q']\n",
    "b_K   = model_named_params['blocks.0.attn.b_K']\n",
    "b_O   = model_named_params['blocks.0.attn.b_O']\n",
    "b_V   = model_named_params['blocks.0.attn.b_V']\n",
    "\n",
    "W_U   = model_named_params['unembed.W_U']\n",
    "b_U   = model_named_params['unembed.b_U']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, let's see if we can figure out a non-confusing way to show the entire projection for a single head for K in a single chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head     = 2\n",
    "\n",
    "# Extract the x and y coordinates from the tensors W_K and b_K\n",
    "# for every head, position, and digit\n",
    "\n",
    "proj_k = np.zeros((model.cfg.n_heads, model.cfg.n_ctx, 10, model.cfg.d_head))\n",
    "for position in range(model.cfg.n_ctx):\n",
    "    proj_k_pos = ((W_E[:10,:] + W_pos[position,np.newaxis]) @ W_K[head,:,:] + b_K[head, np.newaxis,:])\n",
    "    proj_k[head,position,:,:] = proj_k_pos.cpu().detach().numpy()\n",
    "\n",
    "def get_color_for_position(position):\n",
    "    if position < n_digits:\n",
    "        # Positions in first number (a) range from green to blue\n",
    "        start_color, end_color = [0.5, 0.5, 0.0], [0.42, 0.36, 0.80]\n",
    "        frac = position / (n_digits - 1)\n",
    "    elif position < 2*n_digits:\n",
    "        # Positions in second number (b) range from gold to red\n",
    "        start_color, end_color = [0.85, 0.65, 0.13], [1.0, 0.25, 0.0]\n",
    "        frac = (position - n_digits) / (n_digits - 1)\n",
    "    else:\n",
    "        # Positions in result number (a+b) range from pink to purple\n",
    "        start_color, end_color = [1, 0.75, 0.80], [0.5, 0.0, 0.5]\n",
    "        frac = (position - 2*n_digits) / n_digits\n",
    "\n",
    "    color = [(b*frac+a*(1-frac)) for a, b in zip(start_color, end_color)]\n",
    "    return color\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.scatter([], [], c='b', marker='o')    \n",
    "for position in range(model.cfg.n_ctx):\n",
    "    x = proj_k[head,position,:,0]\n",
    "    y = proj_k[head,position,:,1]\n",
    "\n",
    "    color = get_color_for_position(position)\n",
    "\n",
    "    # Add labels for each point to show the order\n",
    "    for digit in range(10):\n",
    "        plt.plot(\n",
    "            proj_k[head,position,[digit,(digit+1)%10],0],\n",
    "            proj_k[head,position,[digit,(digit+1)%10],1],\n",
    "            marker=\"o\",\n",
    "            markersize=4,\n",
    "            linewidth=0.3,\n",
    "            color=color,\n",
    "            **(dict(label=f\"pos={position}\") if digit == 0 else {})\n",
    "        )\n",
    "        plt.annotate(\n",
    "            str(digit),\n",
    "            proj_k[head,position,digit,:],\n",
    "            textcoords=\"offset points\",\n",
    "            xytext=(0,10),\n",
    "            ha='center',\n",
    "            color=color\n",
    "        )\n",
    "\n",
    "for digit in range(10):\n",
    "    for position in range(model.cfg.n_ctx):\n",
    "        if position not in [0, n_digits, n_digits*2]:\n",
    "            color = get_color_for_position(position - 0.5)\n",
    "            plt.plot(\n",
    "                proj_k[head,[position-1,position],digit,0],\n",
    "                proj_k[head,[position-1,position],digit,1],\n",
    "                markersize=4,\n",
    "                linewidth=0.3,\n",
    "                color=color,\n",
    "            )\n",
    "\n",
    "# Set axis labels and title\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.title(f'Projection d_model -> d_head: Attn K: head {head}\\ndigit <- 0..9, position=0')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That chart is pretty confusing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hopefully better charts\n",
    "\n",
    "Let's do it with 2 charts for K (and then another 2 each for Q, V, and B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = dict(\n",
    "    k=np.zeros((model.cfg.n_heads, model.cfg.n_ctx, 10, model.cfg.d_head)),\n",
    "    q=np.zeros((model.cfg.n_heads, model.cfg.n_ctx, 10, model.cfg.d_head)),\n",
    "    o=np.zeros((model.cfg.n_heads, model.cfg.n_ctx, 10, model.cfg.d_head)),\n",
    "    v=np.zeros((model.cfg.n_heads, model.cfg.n_ctx, 10, model.cfg.d_head))\n",
    ")\n",
    "for head in range(model.cfg.n_heads):\n",
    "    for position in range(model.cfg.n_ctx):\n",
    "        proj_k_pos = ((W_E[:10,:] + W_pos[position,np.newaxis]) @ W_K[head,:,:] + b_K[head, np.newaxis,:])\n",
    "        proj_q_pos = ((W_E[:10,:] + W_pos[position,np.newaxis]) @ W_Q[head,:,:] + b_Q[head, np.newaxis,:])\n",
    "        proj_v_pos = ((W_E[:10,:] + W_pos[position,np.newaxis]) @ W_V[head,:,:] + b_V[head, np.newaxis,:])\n",
    "        # Not sure I have the math right here for how to work backwards\n",
    "        # from unembed and W_O to attn space\n",
    "        proj_o_pos = ((W_O[head,:,:]@(W_U[:,:10]) + b_O[head, np.newaxis])).T\n",
    "        proj['k'][head,position,:,:] = proj_k_pos.cpu().detach().numpy()\n",
    "        proj['q'][head,position,:,:] = proj_q_pos.cpu().detach().numpy()\n",
    "        proj['v'][head,position,:,:] = proj_v_pos.cpu().detach().numpy()\n",
    "        proj['o'][head,position,:,:] = proj_o_pos.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that the shapes of the projections make sense (n_heads, d_pos, d_vocab, d_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{k: p.shape for k, p in proj.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_head_projections(head):\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(12, 9))\n",
    "\n",
    "    for ix, param_name in enumerate(\"qkvo\"):\n",
    "        param_proj = proj[param_name]\n",
    "\n",
    "        # Plot the position, for this head, of all projections of q/k/v/o\n",
    "        # into attention space\n",
    "        #  - for each digit for position 0\n",
    "        #  - for each position for digit 0\n",
    "\n",
    "        # Start with digits\n",
    "        ax = axes[0][ix]\n",
    "        ax.set_title(f'Head {head}: attn.{param_name.upper()}\\nd_model -> d_head\\ndigit 0..9, pos=0')\n",
    "        sc = ax.scatter([], [], c='b', marker='o')\n",
    "        axplt = ax.plot(\n",
    "            param_proj[head,0,[d%10 for d in range(11)],0],\n",
    "            param_proj[head,0,[d%10 for d in range(11)],1],\n",
    "            marker=\"o\",\n",
    "            markersize=4,\n",
    "            color=[0.5,0.5,0.5],\n",
    "            linewidth=0.3,\n",
    "        )\n",
    "        for digit in range(10):\n",
    "            ax.annotate(\n",
    "                str(digit),\n",
    "                param_proj[head,0,digit,:],\n",
    "                textcoords=\"offset points\",\n",
    "                xytext=(0,5),\n",
    "                ha='center',\n",
    "            )\n",
    "\n",
    "        # And then do positions\n",
    "        ax = axes[1][ix]\n",
    "        ax.set_title(f'Head {head}: attn.{param_name.upper()}\\nd_model -> d_head\\ndigit 0, pos=0..{model.cfg.n_ctx}')\n",
    "        axplt = ax.plot(\n",
    "            param_proj[head,:,0,0],\n",
    "            param_proj[head,:,0,1],\n",
    "            marker=\"o\",\n",
    "            markersize=4,\n",
    "            color=[0.5,0.5,0.5],\n",
    "            linewidth=0.3,\n",
    "        )\n",
    "        for position in range(model.cfg.n_ctx):\n",
    "            if position < n_digits:\n",
    "                pos_name = f'a:{position}'\n",
    "                color = [0.5, 0.5, 0.0]\n",
    "            elif position < 2*n_digits:\n",
    "                pos_name = f'b:{position-n_digits}'\n",
    "                color = [1.0, 0.25, 0.0]\n",
    "            else:\n",
    "                pos_name = f'b:{position-n_digits}'\n",
    "                color = [1, 0.75, 0.80]\n",
    "\n",
    "            ax.annotate(\n",
    "                str(position),\n",
    "                param_proj[head,position,0,:],\n",
    "                textcoords=\"offset points\",\n",
    "                xytext=(0,5),\n",
    "                ha='center',\n",
    "                color=color\n",
    "            )\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(\n",
    "    show_head_projections,\n",
    "    head=widgets.IntSlider(min=0, max=model.cfg.n_heads-1, step=1, value=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_in_singular_vector_axes(v1,v2,embed):\n",
    "    x = np.dot(embed, v1)\n",
    "    y = np.dot(embed, v2)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(x, y, color='red', label='Point [0, 1]')\n",
    "    plt.plot([0, v1[0]], [0, v1[1]], 'b-', label='Singular vector 1')\n",
    "    plt.plot([0, v2[0]], [0, v2[1]], 'g-', label='Singular vector 2')\n",
    "    plt.xlabel('v1')\n",
    "    plt.ylabel('v2')\n",
    "    plt.title('projected in Custom Coordinate System')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#Demonstrating superposition of attentional features over a head\n",
    "def cosine_sim(x,y):\n",
    "    return torch.dot(x,y) / (torch.norm(x) * torch.norm(y))\n",
    "\n",
    "#Demonstrating superposition of attentional features over a head of dimension 2\n",
    "def svd_trace(model,tokens, N_singular_vectors = 128, sim_threshold = 0.25):\n",
    "  tokens = torch.tensor(tokens).long().cuda()\n",
    "  print(\"Here are the tokens\",tokens[0])\n",
    "  #get the activations of the model\n",
    "  logits, cache = model.run_with_cache(tokens)\n",
    "  embed_token =cache[\"embed\"]\n",
    "  #We do not take into account the positional embeddings\n",
    "  #pos_token = cache['hook_pos_embed']\n",
    "\n",
    "  W_OV = model.W_V @ model.W_O  \n",
    "\n",
    "  #embed_proj = torch.add(embed_token,pos_token)\n",
    "  # normalize\n",
    "  embed_proj = embed_token / torch.sum(embed_token)\n",
    "  \n",
    "  print(\"the shape of tokens[0]\",tokens[0].size(-1))  \n",
    "  # store the sims_values for each head for now only values of one head are being computed  \n",
    "  svd_index_counts = {}\n",
    "  for head in range(model.cfg.n_heads):\n",
    "    \n",
    "      sims_for_each_token = []\n",
    "      for j in range(tokens[0].size(-1)):\n",
    "    \n",
    "          #normalize projected only one token\n",
    "          token_projected = embed_proj[0][j]\n",
    "      \n",
    "          OV_head = W_OV[0][head]\n",
    "          U,S,V = torch.linalg.svd(OV_head)\n",
    "          sims = []\n",
    "        \n",
    "        \n",
    "          for i in range(N_singular_vectors):\n",
    "              sim = cosine_sim(token_projected, V[i,:]).item()\n",
    "              if np.absolute(sim) <=sim_threshold:\n",
    "                  sims.append(0)\n",
    "              else:\n",
    "                  print(\"SIM FOUND:head is \" + str(head) + \" singular vector index is \" + str(i) + \" token is \" + str(tokens[0][j].detach()) + \" sim is \"+ str(sim))\n",
    "                  sims.append(sim)\n",
    "                  svd_index_counts[i] = svd_index_counts.get(i, 0) + 1\n",
    "            \n",
    "          sims = np.array(sims)\n",
    "          sims_for_each_token.append(sims)\n",
    "      sims_for_each_token = np.array(sims_for_each_token)\n",
    "  print(svd_index_counts)\n",
    "  return sims_for_each_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2248/2580868474.py:8: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n",
      "/tmp/ipykernel_2248/1106312766.py:22: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the tokens tensor([6, 5, 4, 3, 2, 1, 9, 8, 7, 0, 0, 0, 5, 4, 2, 4, 2, 1, 0],\n",
      "       device='cuda:0')\n",
      "the shape of tokens[0] 19\n",
      "SIM FOUND:head is 0 singular vector index is 116 token is tensor(4, device='cuda:0') sim is -0.36397314071655273\n",
      "SIM FOUND:head is 0 singular vector index is 113 token is tensor(2, device='cuda:0') sim is 0.25887584686279297\n",
      "SIM FOUND:head is 0 singular vector index is 92 token is tensor(8, device='cuda:0') sim is 0.25981226563453674\n",
      "SIM FOUND:head is 0 singular vector index is 111 token is tensor(8, device='cuda:0') sim is 0.29135745763778687\n",
      "SIM FOUND:head is 0 singular vector index is 80 token is tensor(7, device='cuda:0') sim is 0.256690114736557\n",
      "SIM FOUND:head is 0 singular vector index is 111 token is tensor(7, device='cuda:0') sim is 0.27437877655029297\n",
      "SIM FOUND:head is 0 singular vector index is 116 token is tensor(4, device='cuda:0') sim is -0.36397314071655273\n",
      "SIM FOUND:head is 0 singular vector index is 113 token is tensor(2, device='cuda:0') sim is 0.25887584686279297\n",
      "SIM FOUND:head is 0 singular vector index is 116 token is tensor(4, device='cuda:0') sim is -0.36397314071655273\n",
      "SIM FOUND:head is 0 singular vector index is 113 token is tensor(2, device='cuda:0') sim is 0.25887584686279297\n",
      "SIM FOUND:head is 1 singular vector index is 32 token is tensor(6, device='cuda:0') sim is -0.2941948175430298\n",
      "SIM FOUND:head is 1 singular vector index is 25 token is tensor(5, device='cuda:0') sim is -0.3082248866558075\n",
      "SIM FOUND:head is 1 singular vector index is 118 token is tensor(3, device='cuda:0') sim is 0.2951464354991913\n",
      "SIM FOUND:head is 1 singular vector index is 14 token is tensor(2, device='cuda:0') sim is 0.25964099168777466\n",
      "SIM FOUND:head is 1 singular vector index is 118 token is tensor(2, device='cuda:0') sim is 0.33155879378318787\n",
      "SIM FOUND:head is 1 singular vector index is 119 token is tensor(9, device='cuda:0') sim is -0.2543726861476898\n",
      "SIM FOUND:head is 1 singular vector index is 25 token is tensor(5, device='cuda:0') sim is -0.3082248866558075\n",
      "SIM FOUND:head is 1 singular vector index is 14 token is tensor(2, device='cuda:0') sim is 0.25964099168777466\n",
      "SIM FOUND:head is 1 singular vector index is 118 token is tensor(2, device='cuda:0') sim is 0.33155879378318787\n",
      "SIM FOUND:head is 1 singular vector index is 14 token is tensor(2, device='cuda:0') sim is 0.25964099168777466\n",
      "SIM FOUND:head is 1 singular vector index is 118 token is tensor(2, device='cuda:0') sim is 0.33155879378318787\n",
      "SIM FOUND:head is 2 singular vector index is 83 token is tensor(6, device='cuda:0') sim is -0.2565155625343323\n",
      "SIM FOUND:head is 2 singular vector index is 16 token is tensor(5, device='cuda:0') sim is -0.25090524554252625\n",
      "SIM FOUND:head is 2 singular vector index is 57 token is tensor(2, device='cuda:0') sim is 0.25315025448799133\n",
      "SIM FOUND:head is 2 singular vector index is 57 token is tensor(1, device='cuda:0') sim is 0.2554759681224823\n",
      "SIM FOUND:head is 2 singular vector index is 69 token is tensor(9, device='cuda:0') sim is -0.25559884309768677\n",
      "SIM FOUND:head is 2 singular vector index is 103 token is tensor(7, device='cuda:0') sim is 0.2787626087665558\n",
      "SIM FOUND:head is 2 singular vector index is 16 token is tensor(5, device='cuda:0') sim is -0.25090524554252625\n",
      "SIM FOUND:head is 2 singular vector index is 57 token is tensor(2, device='cuda:0') sim is 0.25315025448799133\n",
      "SIM FOUND:head is 2 singular vector index is 57 token is tensor(2, device='cuda:0') sim is 0.25315025448799133\n",
      "SIM FOUND:head is 2 singular vector index is 57 token is tensor(1, device='cuda:0') sim is 0.2554759681224823\n",
      "SIM FOUND:head is 3 singular vector index is 38 token is tensor(6, device='cuda:0') sim is -0.2922450602054596\n",
      "SIM FOUND:head is 3 singular vector index is 38 token is tensor(5, device='cuda:0') sim is -0.28128185868263245\n",
      "SIM FOUND:head is 3 singular vector index is 64 token is tensor(4, device='cuda:0') sim is 0.26941508054733276\n",
      "SIM FOUND:head is 3 singular vector index is 110 token is tensor(3, device='cuda:0') sim is 0.26828381419181824\n",
      "SIM FOUND:head is 3 singular vector index is 22 token is tensor(2, device='cuda:0') sim is 0.2672933042049408\n",
      "SIM FOUND:head is 3 singular vector index is 42 token is tensor(1, device='cuda:0') sim is 0.2530863583087921\n",
      "SIM FOUND:head is 3 singular vector index is 64 token is tensor(9, device='cuda:0') sim is -0.29173487424850464\n",
      "SIM FOUND:head is 3 singular vector index is 22 token is tensor(8, device='cuda:0') sim is -0.2735861539840698\n",
      "SIM FOUND:head is 3 singular vector index is 64 token is tensor(8, device='cuda:0') sim is -0.29061397910118103\n",
      "SIM FOUND:head is 3 singular vector index is 22 token is tensor(7, device='cuda:0') sim is -0.2608981430530548\n",
      "SIM FOUND:head is 3 singular vector index is 38 token is tensor(5, device='cuda:0') sim is -0.28128185868263245\n",
      "SIM FOUND:head is 3 singular vector index is 64 token is tensor(4, device='cuda:0') sim is 0.26941508054733276\n",
      "SIM FOUND:head is 3 singular vector index is 22 token is tensor(2, device='cuda:0') sim is 0.2672933042049408\n",
      "SIM FOUND:head is 3 singular vector index is 64 token is tensor(4, device='cuda:0') sim is 0.26941508054733276\n",
      "SIM FOUND:head is 3 singular vector index is 22 token is tensor(2, device='cuda:0') sim is 0.2672933042049408\n",
      "SIM FOUND:head is 3 singular vector index is 42 token is tensor(1, device='cuda:0') sim is 0.2530863583087921\n",
      "SIM FOUND:head is 4 singular vector index is 118 token is tensor(5, device='cuda:0') sim is -0.25515511631965637\n",
      "SIM FOUND:head is 4 singular vector index is 38 token is tensor(4, device='cuda:0') sim is 0.25479060411453247\n",
      "SIM FOUND:head is 4 singular vector index is 118 token is tensor(2, device='cuda:0') sim is 0.26322418451309204\n",
      "SIM FOUND:head is 4 singular vector index is 118 token is tensor(5, device='cuda:0') sim is -0.25515511631965637\n",
      "SIM FOUND:head is 4 singular vector index is 38 token is tensor(4, device='cuda:0') sim is 0.25479060411453247\n",
      "SIM FOUND:head is 4 singular vector index is 118 token is tensor(2, device='cuda:0') sim is 0.26322418451309204\n",
      "SIM FOUND:head is 4 singular vector index is 38 token is tensor(4, device='cuda:0') sim is 0.25479060411453247\n",
      "SIM FOUND:head is 4 singular vector index is 118 token is tensor(2, device='cuda:0') sim is 0.26322418451309204\n",
      "SIM FOUND:head is 5 singular vector index is 21 token is tensor(6, device='cuda:0') sim is -0.26610711216926575\n",
      "SIM FOUND:head is 5 singular vector index is 61 token is tensor(6, device='cuda:0') sim is 0.2504233121871948\n",
      "SIM FOUND:head is 5 singular vector index is 108 token is tensor(4, device='cuda:0') sim is 0.28671377897262573\n",
      "SIM FOUND:head is 5 singular vector index is 124 token is tensor(2, device='cuda:0') sim is 0.29099375009536743\n",
      "SIM FOUND:head is 5 singular vector index is 31 token is tensor(1, device='cuda:0') sim is 0.25382691621780396\n",
      "SIM FOUND:head is 5 singular vector index is 92 token is tensor(1, device='cuda:0') sim is -0.2570473253726959\n",
      "SIM FOUND:head is 5 singular vector index is 124 token is tensor(1, device='cuda:0') sim is 0.2542320489883423\n",
      "SIM FOUND:head is 5 singular vector index is 83 token is tensor(9, device='cuda:0') sim is -0.27993258833885193\n",
      "SIM FOUND:head is 5 singular vector index is 41 token is tensor(7, device='cuda:0') sim is -0.2675304710865021\n",
      "SIM FOUND:head is 5 singular vector index is 61 token is tensor(7, device='cuda:0') sim is 0.2628578245639801\n",
      "SIM FOUND:head is 5 singular vector index is 68 token is tensor(0, device='cuda:0') sim is -0.27913907170295715\n",
      "SIM FOUND:head is 5 singular vector index is 68 token is tensor(0, device='cuda:0') sim is -0.27913907170295715\n",
      "SIM FOUND:head is 5 singular vector index is 68 token is tensor(0, device='cuda:0') sim is -0.27913907170295715\n",
      "SIM FOUND:head is 5 singular vector index is 108 token is tensor(4, device='cuda:0') sim is 0.28671377897262573\n",
      "SIM FOUND:head is 5 singular vector index is 124 token is tensor(2, device='cuda:0') sim is 0.29099375009536743\n",
      "SIM FOUND:head is 5 singular vector index is 108 token is tensor(4, device='cuda:0') sim is 0.28671377897262573\n",
      "SIM FOUND:head is 5 singular vector index is 124 token is tensor(2, device='cuda:0') sim is 0.29099375009536743\n",
      "SIM FOUND:head is 5 singular vector index is 31 token is tensor(1, device='cuda:0') sim is 0.25382691621780396\n",
      "SIM FOUND:head is 5 singular vector index is 92 token is tensor(1, device='cuda:0') sim is -0.2570473253726959\n",
      "SIM FOUND:head is 5 singular vector index is 124 token is tensor(1, device='cuda:0') sim is 0.2542320489883423\n",
      "SIM FOUND:head is 5 singular vector index is 68 token is tensor(0, device='cuda:0') sim is -0.27913907170295715\n",
      "SIM FOUND:head is 6 singular vector index is 36 token is tensor(6, device='cuda:0') sim is -0.2758917510509491\n",
      "SIM FOUND:head is 6 singular vector index is 11 token is tensor(5, device='cuda:0') sim is 0.2610565423965454\n",
      "SIM FOUND:head is 6 singular vector index is 29 token is tensor(5, device='cuda:0') sim is -0.26837360858917236\n",
      "SIM FOUND:head is 6 singular vector index is 119 token is tensor(4, device='cuda:0') sim is 0.28060081601142883\n",
      "SIM FOUND:head is 6 singular vector index is 10 token is tensor(3, device='cuda:0') sim is 0.2615421712398529\n",
      "SIM FOUND:head is 6 singular vector index is 12 token is tensor(8, device='cuda:0') sim is -0.27972641587257385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIM FOUND:head is 6 singular vector index is 12 token is tensor(7, device='cuda:0') sim is -0.2605189383029938\n",
      "SIM FOUND:head is 6 singular vector index is 36 token is tensor(7, device='cuda:0') sim is -0.2541234791278839\n",
      "SIM FOUND:head is 6 singular vector index is 90 token is tensor(0, device='cuda:0') sim is -0.2512109875679016\n",
      "SIM FOUND:head is 6 singular vector index is 90 token is tensor(0, device='cuda:0') sim is -0.2512109875679016\n",
      "SIM FOUND:head is 6 singular vector index is 90 token is tensor(0, device='cuda:0') sim is -0.2512109875679016\n",
      "SIM FOUND:head is 6 singular vector index is 11 token is tensor(5, device='cuda:0') sim is 0.2610565423965454\n",
      "SIM FOUND:head is 6 singular vector index is 29 token is tensor(5, device='cuda:0') sim is -0.26837360858917236\n",
      "SIM FOUND:head is 6 singular vector index is 119 token is tensor(4, device='cuda:0') sim is 0.28060081601142883\n",
      "SIM FOUND:head is 6 singular vector index is 119 token is tensor(4, device='cuda:0') sim is 0.28060081601142883\n",
      "SIM FOUND:head is 6 singular vector index is 90 token is tensor(0, device='cuda:0') sim is -0.2512109875679016\n",
      "SIM FOUND:head is 7 singular vector index is 22 token is tensor(5, device='cuda:0') sim is 0.2989756166934967\n",
      "SIM FOUND:head is 7 singular vector index is 97 token is tensor(5, device='cuda:0') sim is 0.26375889778137207\n",
      "SIM FOUND:head is 7 singular vector index is 123 token is tensor(4, device='cuda:0') sim is 0.32927605509757996\n",
      "SIM FOUND:head is 7 singular vector index is 16 token is tensor(1, device='cuda:0') sim is 0.25065046548843384\n",
      "SIM FOUND:head is 7 singular vector index is 31 token is tensor(1, device='cuda:0') sim is -0.2692403793334961\n",
      "SIM FOUND:head is 7 singular vector index is 86 token is tensor(9, device='cuda:0') sim is -0.2622804045677185\n",
      "SIM FOUND:head is 7 singular vector index is 31 token is tensor(7, device='cuda:0') sim is 0.29991430044174194\n",
      "SIM FOUND:head is 7 singular vector index is 22 token is tensor(5, device='cuda:0') sim is 0.2989756166934967\n",
      "SIM FOUND:head is 7 singular vector index is 97 token is tensor(5, device='cuda:0') sim is 0.26375889778137207\n",
      "SIM FOUND:head is 7 singular vector index is 123 token is tensor(4, device='cuda:0') sim is 0.32927605509757996\n",
      "SIM FOUND:head is 7 singular vector index is 123 token is tensor(4, device='cuda:0') sim is 0.32927605509757996\n",
      "SIM FOUND:head is 7 singular vector index is 16 token is tensor(1, device='cuda:0') sim is 0.25065046548843384\n",
      "SIM FOUND:head is 7 singular vector index is 31 token is tensor(1, device='cuda:0') sim is -0.2692403793334961\n",
      "SIM FOUND:head is 8 singular vector index is 56 token is tensor(6, device='cuda:0') sim is -0.31915518641471863\n",
      "SIM FOUND:head is 8 singular vector index is 103 token is tensor(6, device='cuda:0') sim is 0.2537696063518524\n",
      "SIM FOUND:head is 8 singular vector index is 56 token is tensor(5, device='cuda:0') sim is -0.2854137122631073\n",
      "SIM FOUND:head is 8 singular vector index is 103 token is tensor(1, device='cuda:0') sim is -0.2926938235759735\n",
      "SIM FOUND:head is 8 singular vector index is 103 token is tensor(8, device='cuda:0') sim is 0.26201683282852173\n",
      "SIM FOUND:head is 8 singular vector index is 103 token is tensor(7, device='cuda:0') sim is 0.27327075600624084\n",
      "SIM FOUND:head is 8 singular vector index is 103 token is tensor(0, device='cuda:0') sim is -0.27034077048301697\n",
      "SIM FOUND:head is 8 singular vector index is 119 token is tensor(0, device='cuda:0') sim is -0.3021295964717865\n",
      "SIM FOUND:head is 8 singular vector index is 103 token is tensor(0, device='cuda:0') sim is -0.27034077048301697\n",
      "SIM FOUND:head is 8 singular vector index is 119 token is tensor(0, device='cuda:0') sim is -0.3021295964717865\n",
      "SIM FOUND:head is 8 singular vector index is 103 token is tensor(0, device='cuda:0') sim is -0.27034077048301697\n",
      "SIM FOUND:head is 8 singular vector index is 119 token is tensor(0, device='cuda:0') sim is -0.3021295964717865\n",
      "SIM FOUND:head is 8 singular vector index is 56 token is tensor(5, device='cuda:0') sim is -0.2854137122631073\n",
      "SIM FOUND:head is 8 singular vector index is 103 token is tensor(1, device='cuda:0') sim is -0.2926938235759735\n",
      "SIM FOUND:head is 8 singular vector index is 103 token is tensor(0, device='cuda:0') sim is -0.27034077048301697\n",
      "SIM FOUND:head is 8 singular vector index is 119 token is tensor(0, device='cuda:0') sim is -0.3021295964717865\n",
      "SIM FOUND:head is 9 singular vector index is 39 token is tensor(5, device='cuda:0') sim is -0.2743731141090393\n",
      "SIM FOUND:head is 9 singular vector index is 13 token is tensor(3, device='cuda:0') sim is 0.25100061297416687\n",
      "SIM FOUND:head is 9 singular vector index is 78 token is tensor(2, device='cuda:0') sim is -0.26980045437812805\n",
      "SIM FOUND:head is 9 singular vector index is 79 token is tensor(2, device='cuda:0') sim is 0.2653614580631256\n",
      "SIM FOUND:head is 9 singular vector index is 14 token is tensor(8, device='cuda:0') sim is -0.25250038504600525\n",
      "SIM FOUND:head is 9 singular vector index is 122 token is tensor(7, device='cuda:0') sim is 0.2645457684993744\n",
      "SIM FOUND:head is 9 singular vector index is 81 token is tensor(0, device='cuda:0') sim is 0.261683851480484\n",
      "SIM FOUND:head is 9 singular vector index is 81 token is tensor(0, device='cuda:0') sim is 0.261683851480484\n",
      "SIM FOUND:head is 9 singular vector index is 81 token is tensor(0, device='cuda:0') sim is 0.261683851480484\n",
      "SIM FOUND:head is 9 singular vector index is 39 token is tensor(5, device='cuda:0') sim is -0.2743731141090393\n",
      "SIM FOUND:head is 9 singular vector index is 78 token is tensor(2, device='cuda:0') sim is -0.26980045437812805\n",
      "SIM FOUND:head is 9 singular vector index is 79 token is tensor(2, device='cuda:0') sim is 0.2653614580631256\n",
      "SIM FOUND:head is 9 singular vector index is 78 token is tensor(2, device='cuda:0') sim is -0.26980045437812805\n",
      "SIM FOUND:head is 9 singular vector index is 79 token is tensor(2, device='cuda:0') sim is 0.2653614580631256\n",
      "SIM FOUND:head is 9 singular vector index is 81 token is tensor(0, device='cuda:0') sim is 0.261683851480484\n",
      "SIM FOUND:head is 10 singular vector index is 54 token is tensor(5, device='cuda:0') sim is 0.261383980512619\n",
      "SIM FOUND:head is 10 singular vector index is 118 token is tensor(5, device='cuda:0') sim is -0.2691717743873596\n",
      "SIM FOUND:head is 10 singular vector index is 82 token is tensor(4, device='cuda:0') sim is 0.27168354392051697\n",
      "SIM FOUND:head is 10 singular vector index is 15 token is tensor(2, device='cuda:0') sim is -0.26903125643730164\n",
      "SIM FOUND:head is 10 singular vector index is 15 token is tensor(1, device='cuda:0') sim is -0.3397117555141449\n",
      "SIM FOUND:head is 10 singular vector index is 97 token is tensor(1, device='cuda:0') sim is -0.2819197475910187\n",
      "SIM FOUND:head is 10 singular vector index is 9 token is tensor(8, device='cuda:0') sim is 0.2768290638923645\n",
      "SIM FOUND:head is 10 singular vector index is 97 token is tensor(7, device='cuda:0') sim is 0.26349952816963196\n",
      "SIM FOUND:head is 10 singular vector index is 81 token is tensor(0, device='cuda:0') sim is -0.2599908113479614\n",
      "SIM FOUND:head is 10 singular vector index is 81 token is tensor(0, device='cuda:0') sim is -0.2599908113479614\n",
      "SIM FOUND:head is 10 singular vector index is 81 token is tensor(0, device='cuda:0') sim is -0.2599908113479614\n",
      "SIM FOUND:head is 10 singular vector index is 54 token is tensor(5, device='cuda:0') sim is 0.261383980512619\n",
      "SIM FOUND:head is 10 singular vector index is 118 token is tensor(5, device='cuda:0') sim is -0.2691717743873596\n",
      "SIM FOUND:head is 10 singular vector index is 82 token is tensor(4, device='cuda:0') sim is 0.27168354392051697\n",
      "SIM FOUND:head is 10 singular vector index is 15 token is tensor(2, device='cuda:0') sim is -0.26903125643730164\n",
      "SIM FOUND:head is 10 singular vector index is 82 token is tensor(4, device='cuda:0') sim is 0.27168354392051697\n",
      "SIM FOUND:head is 10 singular vector index is 15 token is tensor(2, device='cuda:0') sim is -0.26903125643730164\n",
      "SIM FOUND:head is 10 singular vector index is 15 token is tensor(1, device='cuda:0') sim is -0.3397117555141449\n",
      "SIM FOUND:head is 10 singular vector index is 97 token is tensor(1, device='cuda:0') sim is -0.2819197475910187\n",
      "SIM FOUND:head is 10 singular vector index is 81 token is tensor(0, device='cuda:0') sim is -0.2599908113479614\n",
      "SIM FOUND:head is 11 singular vector index is 112 token is tensor(4, device='cuda:0') sim is 0.25464457273483276\n",
      "SIM FOUND:head is 11 singular vector index is 121 token is tensor(4, device='cuda:0') sim is 0.2658926546573639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIM FOUND:head is 11 singular vector index is 56 token is tensor(2, device='cuda:0') sim is 0.2686837315559387\n",
      "SIM FOUND:head is 11 singular vector index is 69 token is tensor(2, device='cuda:0') sim is 0.2704448699951172\n",
      "SIM FOUND:head is 11 singular vector index is 36 token is tensor(1, device='cuda:0') sim is -0.33291175961494446\n",
      "SIM FOUND:head is 11 singular vector index is 69 token is tensor(1, device='cuda:0') sim is 0.27376043796539307\n",
      "SIM FOUND:head is 11 singular vector index is 64 token is tensor(9, device='cuda:0') sim is 0.28265896439552307\n",
      "SIM FOUND:head is 11 singular vector index is 43 token is tensor(8, device='cuda:0') sim is -0.2620461583137512\n",
      "SIM FOUND:head is 11 singular vector index is 98 token is tensor(8, device='cuda:0') sim is -0.2601041793823242\n",
      "SIM FOUND:head is 11 singular vector index is 112 token is tensor(4, device='cuda:0') sim is 0.25464457273483276\n",
      "SIM FOUND:head is 11 singular vector index is 121 token is tensor(4, device='cuda:0') sim is 0.2658926546573639\n",
      "SIM FOUND:head is 11 singular vector index is 56 token is tensor(2, device='cuda:0') sim is 0.2686837315559387\n",
      "SIM FOUND:head is 11 singular vector index is 69 token is tensor(2, device='cuda:0') sim is 0.2704448699951172\n",
      "SIM FOUND:head is 11 singular vector index is 112 token is tensor(4, device='cuda:0') sim is 0.25464457273483276\n",
      "SIM FOUND:head is 11 singular vector index is 121 token is tensor(4, device='cuda:0') sim is 0.2658926546573639\n",
      "SIM FOUND:head is 11 singular vector index is 56 token is tensor(2, device='cuda:0') sim is 0.2686837315559387\n",
      "SIM FOUND:head is 11 singular vector index is 69 token is tensor(2, device='cuda:0') sim is 0.2704448699951172\n",
      "SIM FOUND:head is 11 singular vector index is 36 token is tensor(1, device='cuda:0') sim is -0.33291175961494446\n",
      "SIM FOUND:head is 11 singular vector index is 69 token is tensor(1, device='cuda:0') sim is 0.27376043796539307\n",
      "SIM FOUND:head is 12 singular vector index is 70 token is tensor(6, device='cuda:0') sim is 0.31702300906181335\n",
      "SIM FOUND:head is 12 singular vector index is 106 token is tensor(6, device='cuda:0') sim is 0.258907288312912\n",
      "SIM FOUND:head is 12 singular vector index is 106 token is tensor(5, device='cuda:0') sim is 0.25956350564956665\n",
      "SIM FOUND:head is 12 singular vector index is 45 token is tensor(4, device='cuda:0') sim is -0.30783161520957947\n",
      "SIM FOUND:head is 12 singular vector index is 75 token is tensor(2, device='cuda:0') sim is -0.25573399662971497\n",
      "SIM FOUND:head is 12 singular vector index is 83 token is tensor(2, device='cuda:0') sim is 0.2899681031703949\n",
      "SIM FOUND:head is 12 singular vector index is 122 token is tensor(2, device='cuda:0') sim is -0.251670241355896\n",
      "SIM FOUND:head is 12 singular vector index is 83 token is tensor(1, device='cuda:0') sim is 0.33913418650627136\n",
      "SIM FOUND:head is 12 singular vector index is 74 token is tensor(9, device='cuda:0') sim is 0.34915804862976074\n",
      "SIM FOUND:head is 12 singular vector index is 51 token is tensor(7, device='cuda:0') sim is 0.2625899910926819\n",
      "SIM FOUND:head is 12 singular vector index is 103 token is tensor(0, device='cuda:0') sim is -0.26477372646331787\n",
      "SIM FOUND:head is 12 singular vector index is 103 token is tensor(0, device='cuda:0') sim is -0.26477372646331787\n",
      "SIM FOUND:head is 12 singular vector index is 103 token is tensor(0, device='cuda:0') sim is -0.26477372646331787\n",
      "SIM FOUND:head is 12 singular vector index is 106 token is tensor(5, device='cuda:0') sim is 0.25956350564956665\n",
      "SIM FOUND:head is 12 singular vector index is 45 token is tensor(4, device='cuda:0') sim is -0.30783161520957947\n",
      "SIM FOUND:head is 12 singular vector index is 75 token is tensor(2, device='cuda:0') sim is -0.25573399662971497\n",
      "SIM FOUND:head is 12 singular vector index is 83 token is tensor(2, device='cuda:0') sim is 0.2899681031703949\n",
      "SIM FOUND:head is 12 singular vector index is 122 token is tensor(2, device='cuda:0') sim is -0.251670241355896\n",
      "SIM FOUND:head is 12 singular vector index is 45 token is tensor(4, device='cuda:0') sim is -0.30783161520957947\n",
      "SIM FOUND:head is 12 singular vector index is 75 token is tensor(2, device='cuda:0') sim is -0.25573399662971497\n",
      "SIM FOUND:head is 12 singular vector index is 83 token is tensor(2, device='cuda:0') sim is 0.2899681031703949\n",
      "SIM FOUND:head is 12 singular vector index is 122 token is tensor(2, device='cuda:0') sim is -0.251670241355896\n",
      "SIM FOUND:head is 12 singular vector index is 83 token is tensor(1, device='cuda:0') sim is 0.33913418650627136\n",
      "SIM FOUND:head is 12 singular vector index is 103 token is tensor(0, device='cuda:0') sim is -0.26477372646331787\n",
      "SIM FOUND:head is 13 singular vector index is 95 token is tensor(5, device='cuda:0') sim is -0.27854180335998535\n",
      "SIM FOUND:head is 13 singular vector index is 24 token is tensor(3, device='cuda:0') sim is 0.26792341470718384\n",
      "SIM FOUND:head is 13 singular vector index is 52 token is tensor(3, device='cuda:0') sim is 0.25079602003097534\n",
      "SIM FOUND:head is 13 singular vector index is 39 token is tensor(2, device='cuda:0') sim is -0.2694685459136963\n",
      "SIM FOUND:head is 13 singular vector index is 95 token is tensor(5, device='cuda:0') sim is -0.27854180335998535\n",
      "SIM FOUND:head is 13 singular vector index is 39 token is tensor(2, device='cuda:0') sim is -0.2694685459136963\n",
      "SIM FOUND:head is 13 singular vector index is 39 token is tensor(2, device='cuda:0') sim is -0.2694685459136963\n",
      "SIM FOUND:head is 14 singular vector index is 110 token is tensor(6, device='cuda:0') sim is -0.27945181727409363\n",
      "SIM FOUND:head is 14 singular vector index is 80 token is tensor(4, device='cuda:0') sim is 0.2519107162952423\n",
      "SIM FOUND:head is 14 singular vector index is 119 token is tensor(4, device='cuda:0') sim is 0.2542368173599243\n",
      "SIM FOUND:head is 14 singular vector index is 109 token is tensor(2, device='cuda:0') sim is 0.25029346346855164\n",
      "SIM FOUND:head is 14 singular vector index is 110 token is tensor(2, device='cuda:0') sim is 0.28274425864219666\n",
      "SIM FOUND:head is 14 singular vector index is 21 token is tensor(9, device='cuda:0') sim is -0.2611047327518463\n",
      "SIM FOUND:head is 14 singular vector index is 93 token is tensor(9, device='cuda:0') sim is 0.2680651843547821\n",
      "SIM FOUND:head is 14 singular vector index is 110 token is tensor(7, device='cuda:0') sim is -0.2500455379486084\n",
      "SIM FOUND:head is 14 singular vector index is 40 token is tensor(0, device='cuda:0') sim is 0.2962247431278229\n",
      "SIM FOUND:head is 14 singular vector index is 40 token is tensor(0, device='cuda:0') sim is 0.2962247431278229\n",
      "SIM FOUND:head is 14 singular vector index is 40 token is tensor(0, device='cuda:0') sim is 0.2962247431278229\n",
      "SIM FOUND:head is 14 singular vector index is 80 token is tensor(4, device='cuda:0') sim is 0.2519107162952423\n",
      "SIM FOUND:head is 14 singular vector index is 119 token is tensor(4, device='cuda:0') sim is 0.2542368173599243\n",
      "SIM FOUND:head is 14 singular vector index is 109 token is tensor(2, device='cuda:0') sim is 0.25029346346855164\n",
      "SIM FOUND:head is 14 singular vector index is 110 token is tensor(2, device='cuda:0') sim is 0.28274425864219666\n",
      "SIM FOUND:head is 14 singular vector index is 80 token is tensor(4, device='cuda:0') sim is 0.2519107162952423\n",
      "SIM FOUND:head is 14 singular vector index is 119 token is tensor(4, device='cuda:0') sim is 0.2542368173599243\n",
      "SIM FOUND:head is 14 singular vector index is 109 token is tensor(2, device='cuda:0') sim is 0.25029346346855164\n",
      "SIM FOUND:head is 14 singular vector index is 110 token is tensor(2, device='cuda:0') sim is 0.28274425864219666\n",
      "SIM FOUND:head is 14 singular vector index is 40 token is tensor(0, device='cuda:0') sim is 0.2962247431278229\n",
      "SIM FOUND:head is 15 singular vector index is 60 token is tensor(3, device='cuda:0') sim is 0.31273162364959717\n",
      "SIM FOUND:head is 15 singular vector index is 60 token is tensor(2, device='cuda:0') sim is 0.27672797441482544\n",
      "SIM FOUND:head is 15 singular vector index is 27 token is tensor(9, device='cuda:0') sim is 0.2533472180366516\n",
      "SIM FOUND:head is 15 singular vector index is 29 token is tensor(8, device='cuda:0') sim is 0.2503722310066223\n",
      "SIM FOUND:head is 15 singular vector index is 28 token is tensor(7, device='cuda:0') sim is 0.2719143033027649\n",
      "SIM FOUND:head is 15 singular vector index is 60 token is tensor(2, device='cuda:0') sim is 0.27672797441482544\n",
      "SIM FOUND:head is 15 singular vector index is 60 token is tensor(2, device='cuda:0') sim is 0.27672797441482544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIM FOUND:head is 16 singular vector index is 106 token is tensor(3, device='cuda:0') sim is -0.35353899002075195\n",
      "SIM FOUND:head is 16 singular vector index is 124 token is tensor(2, device='cuda:0') sim is -0.319429486989975\n",
      "SIM FOUND:head is 16 singular vector index is 12 token is tensor(1, device='cuda:0') sim is 0.2617831826210022\n",
      "SIM FOUND:head is 16 singular vector index is 124 token is tensor(1, device='cuda:0') sim is -0.28469565510749817\n",
      "SIM FOUND:head is 16 singular vector index is 68 token is tensor(9, device='cuda:0') sim is 0.25410202145576477\n",
      "SIM FOUND:head is 16 singular vector index is 12 token is tensor(0, device='cuda:0') sim is 0.2863026261329651\n",
      "SIM FOUND:head is 16 singular vector index is 13 token is tensor(0, device='cuda:0') sim is 0.259422242641449\n",
      "SIM FOUND:head is 16 singular vector index is 12 token is tensor(0, device='cuda:0') sim is 0.2863026261329651\n",
      "SIM FOUND:head is 16 singular vector index is 13 token is tensor(0, device='cuda:0') sim is 0.259422242641449\n",
      "SIM FOUND:head is 16 singular vector index is 12 token is tensor(0, device='cuda:0') sim is 0.2863026261329651\n",
      "SIM FOUND:head is 16 singular vector index is 13 token is tensor(0, device='cuda:0') sim is 0.259422242641449\n",
      "SIM FOUND:head is 16 singular vector index is 124 token is tensor(2, device='cuda:0') sim is -0.319429486989975\n",
      "SIM FOUND:head is 16 singular vector index is 124 token is tensor(2, device='cuda:0') sim is -0.319429486989975\n",
      "SIM FOUND:head is 16 singular vector index is 12 token is tensor(1, device='cuda:0') sim is 0.2617831826210022\n",
      "SIM FOUND:head is 16 singular vector index is 124 token is tensor(1, device='cuda:0') sim is -0.28469565510749817\n",
      "SIM FOUND:head is 16 singular vector index is 12 token is tensor(0, device='cuda:0') sim is 0.2863026261329651\n",
      "SIM FOUND:head is 16 singular vector index is 13 token is tensor(0, device='cuda:0') sim is 0.259422242641449\n",
      "SIM FOUND:head is 17 singular vector index is 29 token is tensor(4, device='cuda:0') sim is 0.253003865480423\n",
      "SIM FOUND:head is 17 singular vector index is 67 token is tensor(2, device='cuda:0') sim is -0.25350600481033325\n",
      "SIM FOUND:head is 17 singular vector index is 102 token is tensor(2, device='cuda:0') sim is -0.2709592580795288\n",
      "SIM FOUND:head is 17 singular vector index is 92 token is tensor(1, device='cuda:0') sim is 0.2895508110523224\n",
      "SIM FOUND:head is 17 singular vector index is 127 token is tensor(9, device='cuda:0') sim is -0.2650219798088074\n",
      "SIM FOUND:head is 17 singular vector index is 92 token is tensor(0, device='cuda:0') sim is 0.25901204347610474\n",
      "SIM FOUND:head is 17 singular vector index is 92 token is tensor(0, device='cuda:0') sim is 0.25901204347610474\n",
      "SIM FOUND:head is 17 singular vector index is 92 token is tensor(0, device='cuda:0') sim is 0.25901204347610474\n",
      "SIM FOUND:head is 17 singular vector index is 29 token is tensor(4, device='cuda:0') sim is 0.253003865480423\n",
      "SIM FOUND:head is 17 singular vector index is 67 token is tensor(2, device='cuda:0') sim is -0.25350600481033325\n",
      "SIM FOUND:head is 17 singular vector index is 102 token is tensor(2, device='cuda:0') sim is -0.2709592580795288\n",
      "SIM FOUND:head is 17 singular vector index is 29 token is tensor(4, device='cuda:0') sim is 0.253003865480423\n",
      "SIM FOUND:head is 17 singular vector index is 67 token is tensor(2, device='cuda:0') sim is -0.25350600481033325\n",
      "SIM FOUND:head is 17 singular vector index is 102 token is tensor(2, device='cuda:0') sim is -0.2709592580795288\n",
      "SIM FOUND:head is 17 singular vector index is 92 token is tensor(1, device='cuda:0') sim is 0.2895508110523224\n",
      "SIM FOUND:head is 17 singular vector index is 92 token is tensor(0, device='cuda:0') sim is 0.25901204347610474\n",
      "SIM FOUND:head is 18 singular vector index is 50 token is tensor(5, device='cuda:0') sim is -0.25142255425453186\n",
      "SIM FOUND:head is 18 singular vector index is 121 token is tensor(5, device='cuda:0') sim is -0.27611294388771057\n",
      "SIM FOUND:head is 18 singular vector index is 65 token is tensor(4, device='cuda:0') sim is -0.274794340133667\n",
      "SIM FOUND:head is 18 singular vector index is 94 token is tensor(2, device='cuda:0') sim is -0.29568248987197876\n",
      "SIM FOUND:head is 18 singular vector index is 16 token is tensor(1, device='cuda:0') sim is -0.26644963026046753\n",
      "SIM FOUND:head is 18 singular vector index is 94 token is tensor(1, device='cuda:0') sim is -0.30082350969314575\n",
      "SIM FOUND:head is 18 singular vector index is 113 token is tensor(9, device='cuda:0') sim is -0.29340386390686035\n",
      "SIM FOUND:head is 18 singular vector index is 16 token is tensor(8, device='cuda:0') sim is 0.28951624035835266\n",
      "SIM FOUND:head is 18 singular vector index is 16 token is tensor(7, device='cuda:0') sim is 0.3517690598964691\n",
      "SIM FOUND:head is 18 singular vector index is 82 token is tensor(0, device='cuda:0') sim is -0.25492724776268005\n",
      "SIM FOUND:head is 18 singular vector index is 82 token is tensor(0, device='cuda:0') sim is -0.25492724776268005\n",
      "SIM FOUND:head is 18 singular vector index is 82 token is tensor(0, device='cuda:0') sim is -0.25492724776268005\n",
      "SIM FOUND:head is 18 singular vector index is 50 token is tensor(5, device='cuda:0') sim is -0.25142255425453186\n",
      "SIM FOUND:head is 18 singular vector index is 121 token is tensor(5, device='cuda:0') sim is -0.27611294388771057\n",
      "SIM FOUND:head is 18 singular vector index is 65 token is tensor(4, device='cuda:0') sim is -0.274794340133667\n",
      "SIM FOUND:head is 18 singular vector index is 94 token is tensor(2, device='cuda:0') sim is -0.29568248987197876\n",
      "SIM FOUND:head is 18 singular vector index is 65 token is tensor(4, device='cuda:0') sim is -0.274794340133667\n",
      "SIM FOUND:head is 18 singular vector index is 94 token is tensor(2, device='cuda:0') sim is -0.29568248987197876\n",
      "SIM FOUND:head is 18 singular vector index is 16 token is tensor(1, device='cuda:0') sim is -0.26644963026046753\n",
      "SIM FOUND:head is 18 singular vector index is 94 token is tensor(1, device='cuda:0') sim is -0.30082350969314575\n",
      "SIM FOUND:head is 18 singular vector index is 82 token is tensor(0, device='cuda:0') sim is -0.25492724776268005\n",
      "SIM FOUND:head is 19 singular vector index is 114 token is tensor(5, device='cuda:0') sim is 0.2524750232696533\n",
      "SIM FOUND:head is 19 singular vector index is 71 token is tensor(4, device='cuda:0') sim is 0.2726413905620575\n",
      "SIM FOUND:head is 19 singular vector index is 105 token is tensor(3, device='cuda:0') sim is 0.3074340522289276\n",
      "SIM FOUND:head is 19 singular vector index is 76 token is tensor(9, device='cuda:0') sim is -0.32093551754951477\n",
      "SIM FOUND:head is 19 singular vector index is 105 token is tensor(0, device='cuda:0') sim is -0.253549724817276\n",
      "SIM FOUND:head is 19 singular vector index is 105 token is tensor(0, device='cuda:0') sim is -0.253549724817276\n",
      "SIM FOUND:head is 19 singular vector index is 105 token is tensor(0, device='cuda:0') sim is -0.253549724817276\n",
      "SIM FOUND:head is 19 singular vector index is 114 token is tensor(5, device='cuda:0') sim is 0.2524750232696533\n",
      "SIM FOUND:head is 19 singular vector index is 71 token is tensor(4, device='cuda:0') sim is 0.2726413905620575\n",
      "SIM FOUND:head is 19 singular vector index is 71 token is tensor(4, device='cuda:0') sim is 0.2726413905620575\n",
      "SIM FOUND:head is 19 singular vector index is 105 token is tensor(0, device='cuda:0') sim is -0.253549724817276\n",
      "SIM FOUND:head is 20 singular vector index is 104 token is tensor(4, device='cuda:0') sim is 0.2725262939929962\n",
      "SIM FOUND:head is 20 singular vector index is 109 token is tensor(4, device='cuda:0') sim is 0.27004268765449524\n",
      "SIM FOUND:head is 20 singular vector index is 69 token is tensor(2, device='cuda:0') sim is 0.2690953016281128\n",
      "SIM FOUND:head is 20 singular vector index is 68 token is tensor(9, device='cuda:0') sim is 0.2638694941997528\n",
      "SIM FOUND:head is 20 singular vector index is 8 token is tensor(8, device='cuda:0') sim is -0.255933552980423\n",
      "SIM FOUND:head is 20 singular vector index is 40 token is tensor(8, device='cuda:0') sim is 0.25245392322540283\n",
      "SIM FOUND:head is 20 singular vector index is 104 token is tensor(4, device='cuda:0') sim is 0.2725262939929962\n",
      "SIM FOUND:head is 20 singular vector index is 109 token is tensor(4, device='cuda:0') sim is 0.27004268765449524\n",
      "SIM FOUND:head is 20 singular vector index is 69 token is tensor(2, device='cuda:0') sim is 0.2690953016281128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIM FOUND:head is 20 singular vector index is 104 token is tensor(4, device='cuda:0') sim is 0.2725262939929962\n",
      "SIM FOUND:head is 20 singular vector index is 109 token is tensor(4, device='cuda:0') sim is 0.27004268765449524\n",
      "SIM FOUND:head is 20 singular vector index is 69 token is tensor(2, device='cuda:0') sim is 0.2690953016281128\n",
      "SIM FOUND:head is 21 singular vector index is 58 token is tensor(6, device='cuda:0') sim is -0.2601154148578644\n",
      "SIM FOUND:head is 21 singular vector index is 96 token is tensor(6, device='cuda:0') sim is -0.27015894651412964\n",
      "SIM FOUND:head is 21 singular vector index is 82 token is tensor(4, device='cuda:0') sim is -0.2651817202568054\n",
      "SIM FOUND:head is 21 singular vector index is 57 token is tensor(1, device='cuda:0') sim is -0.2557094097137451\n",
      "SIM FOUND:head is 21 singular vector index is 27 token is tensor(9, device='cuda:0') sim is -0.2508973479270935\n",
      "SIM FOUND:head is 21 singular vector index is 82 token is tensor(8, device='cuda:0') sim is 0.3243309557437897\n",
      "SIM FOUND:head is 21 singular vector index is 82 token is tensor(7, device='cuda:0') sim is 0.27733054757118225\n",
      "SIM FOUND:head is 21 singular vector index is 101 token is tensor(0, device='cuda:0') sim is -0.25942450761795044\n",
      "SIM FOUND:head is 21 singular vector index is 117 token is tensor(0, device='cuda:0') sim is -0.27223825454711914\n",
      "SIM FOUND:head is 21 singular vector index is 101 token is tensor(0, device='cuda:0') sim is -0.25942450761795044\n",
      "SIM FOUND:head is 21 singular vector index is 117 token is tensor(0, device='cuda:0') sim is -0.27223825454711914\n",
      "SIM FOUND:head is 21 singular vector index is 101 token is tensor(0, device='cuda:0') sim is -0.25942450761795044\n",
      "SIM FOUND:head is 21 singular vector index is 117 token is tensor(0, device='cuda:0') sim is -0.27223825454711914\n",
      "SIM FOUND:head is 21 singular vector index is 82 token is tensor(4, device='cuda:0') sim is -0.2651817202568054\n",
      "SIM FOUND:head is 21 singular vector index is 82 token is tensor(4, device='cuda:0') sim is -0.2651817202568054\n",
      "SIM FOUND:head is 21 singular vector index is 57 token is tensor(1, device='cuda:0') sim is -0.2557094097137451\n",
      "SIM FOUND:head is 21 singular vector index is 101 token is tensor(0, device='cuda:0') sim is -0.25942450761795044\n",
      "SIM FOUND:head is 21 singular vector index is 117 token is tensor(0, device='cuda:0') sim is -0.27223825454711914\n",
      "SIM FOUND:head is 22 singular vector index is 121 token is tensor(4, device='cuda:0') sim is -0.260237455368042\n",
      "SIM FOUND:head is 22 singular vector index is 112 token is tensor(2, device='cuda:0') sim is -0.3011287450790405\n",
      "SIM FOUND:head is 22 singular vector index is 88 token is tensor(1, device='cuda:0') sim is 0.26705262064933777\n",
      "SIM FOUND:head is 22 singular vector index is 119 token is tensor(9, device='cuda:0') sim is 0.26147642731666565\n",
      "SIM FOUND:head is 22 singular vector index is 40 token is tensor(0, device='cuda:0') sim is -0.29625439643859863\n",
      "SIM FOUND:head is 22 singular vector index is 40 token is tensor(0, device='cuda:0') sim is -0.29625439643859863\n",
      "SIM FOUND:head is 22 singular vector index is 40 token is tensor(0, device='cuda:0') sim is -0.29625439643859863\n",
      "SIM FOUND:head is 22 singular vector index is 121 token is tensor(4, device='cuda:0') sim is -0.260237455368042\n",
      "SIM FOUND:head is 22 singular vector index is 112 token is tensor(2, device='cuda:0') sim is -0.3011287450790405\n",
      "SIM FOUND:head is 22 singular vector index is 121 token is tensor(4, device='cuda:0') sim is -0.260237455368042\n",
      "SIM FOUND:head is 22 singular vector index is 112 token is tensor(2, device='cuda:0') sim is -0.3011287450790405\n",
      "SIM FOUND:head is 22 singular vector index is 88 token is tensor(1, device='cuda:0') sim is 0.26705262064933777\n",
      "SIM FOUND:head is 22 singular vector index is 40 token is tensor(0, device='cuda:0') sim is -0.29625439643859863\n",
      "SIM FOUND:head is 23 singular vector index is 57 token is tensor(6, device='cuda:0') sim is -0.2985447347164154\n",
      "SIM FOUND:head is 23 singular vector index is 58 token is tensor(6, device='cuda:0') sim is -0.2625269591808319\n",
      "SIM FOUND:head is 23 singular vector index is 24 token is tensor(5, device='cuda:0') sim is -0.32189351320266724\n",
      "SIM FOUND:head is 23 singular vector index is 85 token is tensor(4, device='cuda:0') sim is -0.25256913900375366\n",
      "SIM FOUND:head is 23 singular vector index is 86 token is tensor(4, device='cuda:0') sim is 0.2561904788017273\n",
      "SIM FOUND:head is 23 singular vector index is 57 token is tensor(2, device='cuda:0') sim is 0.3259447515010834\n",
      "SIM FOUND:head is 23 singular vector index is 44 token is tensor(1, device='cuda:0') sim is 0.28080323338508606\n",
      "SIM FOUND:head is 23 singular vector index is 57 token is tensor(7, device='cuda:0') sim is -0.2894727885723114\n",
      "SIM FOUND:head is 23 singular vector index is 44 token is tensor(0, device='cuda:0') sim is 0.2502748668193817\n",
      "SIM FOUND:head is 23 singular vector index is 65 token is tensor(0, device='cuda:0') sim is 0.25787344574928284\n",
      "SIM FOUND:head is 23 singular vector index is 120 token is tensor(0, device='cuda:0') sim is 0.27110642194747925\n",
      "SIM FOUND:head is 23 singular vector index is 44 token is tensor(0, device='cuda:0') sim is 0.2502748668193817\n",
      "SIM FOUND:head is 23 singular vector index is 65 token is tensor(0, device='cuda:0') sim is 0.25787344574928284\n",
      "SIM FOUND:head is 23 singular vector index is 120 token is tensor(0, device='cuda:0') sim is 0.27110642194747925\n",
      "SIM FOUND:head is 23 singular vector index is 44 token is tensor(0, device='cuda:0') sim is 0.2502748668193817\n",
      "SIM FOUND:head is 23 singular vector index is 65 token is tensor(0, device='cuda:0') sim is 0.25787344574928284\n",
      "SIM FOUND:head is 23 singular vector index is 120 token is tensor(0, device='cuda:0') sim is 0.27110642194747925\n",
      "SIM FOUND:head is 23 singular vector index is 24 token is tensor(5, device='cuda:0') sim is -0.32189351320266724\n",
      "SIM FOUND:head is 23 singular vector index is 85 token is tensor(4, device='cuda:0') sim is -0.25256913900375366\n",
      "SIM FOUND:head is 23 singular vector index is 86 token is tensor(4, device='cuda:0') sim is 0.2561904788017273\n",
      "SIM FOUND:head is 23 singular vector index is 57 token is tensor(2, device='cuda:0') sim is 0.3259447515010834\n",
      "SIM FOUND:head is 23 singular vector index is 85 token is tensor(4, device='cuda:0') sim is -0.25256913900375366\n",
      "SIM FOUND:head is 23 singular vector index is 86 token is tensor(4, device='cuda:0') sim is 0.2561904788017273\n",
      "SIM FOUND:head is 23 singular vector index is 57 token is tensor(2, device='cuda:0') sim is 0.3259447515010834\n",
      "SIM FOUND:head is 23 singular vector index is 44 token is tensor(1, device='cuda:0') sim is 0.28080323338508606\n",
      "SIM FOUND:head is 23 singular vector index is 44 token is tensor(0, device='cuda:0') sim is 0.2502748668193817\n",
      "SIM FOUND:head is 23 singular vector index is 65 token is tensor(0, device='cuda:0') sim is 0.25787344574928284\n",
      "SIM FOUND:head is 23 singular vector index is 120 token is tensor(0, device='cuda:0') sim is 0.27110642194747925\n",
      "SIM FOUND:head is 24 singular vector index is 30 token is tensor(5, device='cuda:0') sim is 0.26122161746025085\n",
      "SIM FOUND:head is 24 singular vector index is 66 token is tensor(5, device='cuda:0') sim is -0.26794084906578064\n",
      "SIM FOUND:head is 24 singular vector index is 111 token is tensor(3, device='cuda:0') sim is 0.2555316686630249\n",
      "SIM FOUND:head is 24 singular vector index is 14 token is tensor(8, device='cuda:0') sim is 0.2637670636177063\n",
      "SIM FOUND:head is 24 singular vector index is 14 token is tensor(0, device='cuda:0') sim is -0.25370049476623535\n",
      "SIM FOUND:head is 24 singular vector index is 28 token is tensor(0, device='cuda:0') sim is -0.3105209767818451\n",
      "SIM FOUND:head is 24 singular vector index is 14 token is tensor(0, device='cuda:0') sim is -0.25370049476623535\n",
      "SIM FOUND:head is 24 singular vector index is 28 token is tensor(0, device='cuda:0') sim is -0.3105209767818451\n",
      "SIM FOUND:head is 24 singular vector index is 14 token is tensor(0, device='cuda:0') sim is -0.25370049476623535\n",
      "SIM FOUND:head is 24 singular vector index is 28 token is tensor(0, device='cuda:0') sim is -0.3105209767818451\n",
      "SIM FOUND:head is 24 singular vector index is 30 token is tensor(5, device='cuda:0') sim is 0.26122161746025085\n",
      "SIM FOUND:head is 24 singular vector index is 66 token is tensor(5, device='cuda:0') sim is -0.26794084906578064\n",
      "SIM FOUND:head is 24 singular vector index is 14 token is tensor(0, device='cuda:0') sim is -0.25370049476623535\n",
      "SIM FOUND:head is 24 singular vector index is 28 token is tensor(0, device='cuda:0') sim is -0.3105209767818451\n",
      "SIM FOUND:head is 25 singular vector index is 77 token is tensor(3, device='cuda:0') sim is -0.25223222374916077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIM FOUND:head is 25 singular vector index is 110 token is tensor(2, device='cuda:0') sim is -0.2584320306777954\n",
      "SIM FOUND:head is 25 singular vector index is 92 token is tensor(1, device='cuda:0') sim is -0.2569356858730316\n",
      "SIM FOUND:head is 25 singular vector index is 127 token is tensor(0, device='cuda:0') sim is 0.25017642974853516\n",
      "SIM FOUND:head is 25 singular vector index is 127 token is tensor(0, device='cuda:0') sim is 0.25017642974853516\n",
      "SIM FOUND:head is 25 singular vector index is 127 token is tensor(0, device='cuda:0') sim is 0.25017642974853516\n",
      "SIM FOUND:head is 25 singular vector index is 110 token is tensor(2, device='cuda:0') sim is -0.2584320306777954\n",
      "SIM FOUND:head is 25 singular vector index is 110 token is tensor(2, device='cuda:0') sim is -0.2584320306777954\n",
      "SIM FOUND:head is 25 singular vector index is 92 token is tensor(1, device='cuda:0') sim is -0.2569356858730316\n",
      "SIM FOUND:head is 25 singular vector index is 127 token is tensor(0, device='cuda:0') sim is 0.25017642974853516\n",
      "SIM FOUND:head is 26 singular vector index is 37 token is tensor(6, device='cuda:0') sim is -0.26185327768325806\n",
      "SIM FOUND:head is 26 singular vector index is 117 token is tensor(6, device='cuda:0') sim is -0.3127903342247009\n",
      "SIM FOUND:head is 26 singular vector index is 47 token is tensor(5, device='cuda:0') sim is -0.27426281571388245\n",
      "SIM FOUND:head is 26 singular vector index is 108 token is tensor(1, device='cuda:0') sim is 0.27863675355911255\n",
      "SIM FOUND:head is 26 singular vector index is 47 token is tensor(5, device='cuda:0') sim is -0.27426281571388245\n",
      "SIM FOUND:head is 26 singular vector index is 108 token is tensor(1, device='cuda:0') sim is 0.27863675355911255\n",
      "SIM FOUND:head is 27 singular vector index is 123 token is tensor(3, device='cuda:0') sim is -0.3054775893688202\n",
      "SIM FOUND:head is 27 singular vector index is 57 token is tensor(7, device='cuda:0') sim is -0.25443413853645325\n",
      "SIM FOUND:head is 27 singular vector index is 126 token is tensor(7, device='cuda:0') sim is -0.29737648367881775\n",
      "SIM FOUND:head is 27 singular vector index is 122 token is tensor(0, device='cuda:0') sim is 0.27353543043136597\n",
      "SIM FOUND:head is 27 singular vector index is 122 token is tensor(0, device='cuda:0') sim is 0.27353543043136597\n",
      "SIM FOUND:head is 27 singular vector index is 122 token is tensor(0, device='cuda:0') sim is 0.27353543043136597\n",
      "SIM FOUND:head is 27 singular vector index is 122 token is tensor(0, device='cuda:0') sim is 0.27353543043136597\n",
      "SIM FOUND:head is 28 singular vector index is 83 token is tensor(2, device='cuda:0') sim is -0.2694135010242462\n",
      "SIM FOUND:head is 28 singular vector index is 119 token is tensor(2, device='cuda:0') sim is 0.31151971220970154\n",
      "SIM FOUND:head is 28 singular vector index is 119 token is tensor(1, device='cuda:0') sim is 0.26807597279548645\n",
      "SIM FOUND:head is 28 singular vector index is 83 token is tensor(8, device='cuda:0') sim is 0.2659381628036499\n",
      "SIM FOUND:head is 28 singular vector index is 119 token is tensor(8, device='cuda:0') sim is -0.2657547891139984\n",
      "SIM FOUND:head is 28 singular vector index is 103 token is tensor(7, device='cuda:0') sim is -0.2952316105365753\n",
      "SIM FOUND:head is 28 singular vector index is 83 token is tensor(2, device='cuda:0') sim is -0.2694135010242462\n",
      "SIM FOUND:head is 28 singular vector index is 119 token is tensor(2, device='cuda:0') sim is 0.31151971220970154\n",
      "SIM FOUND:head is 28 singular vector index is 83 token is tensor(2, device='cuda:0') sim is -0.2694135010242462\n",
      "SIM FOUND:head is 28 singular vector index is 119 token is tensor(2, device='cuda:0') sim is 0.31151971220970154\n",
      "SIM FOUND:head is 28 singular vector index is 119 token is tensor(1, device='cuda:0') sim is 0.26807597279548645\n",
      "SIM FOUND:head is 29 singular vector index is 52 token is tensor(6, device='cuda:0') sim is -0.27578121423721313\n",
      "SIM FOUND:head is 29 singular vector index is 52 token is tensor(5, device='cuda:0') sim is -0.27177000045776367\n",
      "SIM FOUND:head is 29 singular vector index is 25 token is tensor(1, device='cuda:0') sim is -0.25715532898902893\n",
      "SIM FOUND:head is 29 singular vector index is 27 token is tensor(9, device='cuda:0') sim is 0.25449317693710327\n",
      "SIM FOUND:head is 29 singular vector index is 118 token is tensor(8, device='cuda:0') sim is -0.2917223572731018\n",
      "SIM FOUND:head is 29 singular vector index is 78 token is tensor(7, device='cuda:0') sim is -0.27106326818466187\n",
      "SIM FOUND:head is 29 singular vector index is 118 token is tensor(7, device='cuda:0') sim is -0.3734295070171356\n",
      "SIM FOUND:head is 29 singular vector index is 25 token is tensor(0, device='cuda:0') sim is -0.2603100538253784\n",
      "SIM FOUND:head is 29 singular vector index is 25 token is tensor(0, device='cuda:0') sim is -0.2603100538253784\n",
      "SIM FOUND:head is 29 singular vector index is 25 token is tensor(0, device='cuda:0') sim is -0.2603100538253784\n",
      "SIM FOUND:head is 29 singular vector index is 52 token is tensor(5, device='cuda:0') sim is -0.27177000045776367\n",
      "SIM FOUND:head is 29 singular vector index is 25 token is tensor(1, device='cuda:0') sim is -0.25715532898902893\n",
      "SIM FOUND:head is 29 singular vector index is 25 token is tensor(0, device='cuda:0') sim is -0.2603100538253784\n",
      "SIM FOUND:head is 30 singular vector index is 93 token is tensor(6, device='cuda:0') sim is -0.27642595767974854\n",
      "SIM FOUND:head is 30 singular vector index is 106 token is tensor(4, device='cuda:0') sim is 0.2807904779911041\n",
      "SIM FOUND:head is 30 singular vector index is 20 token is tensor(9, device='cuda:0') sim is 0.2567898631095886\n",
      "SIM FOUND:head is 30 singular vector index is 93 token is tensor(7, device='cuda:0') sim is -0.25857052206993103\n",
      "SIM FOUND:head is 30 singular vector index is 113 token is tensor(0, device='cuda:0') sim is 0.2512510120868683\n",
      "SIM FOUND:head is 30 singular vector index is 113 token is tensor(0, device='cuda:0') sim is 0.2512510120868683\n",
      "SIM FOUND:head is 30 singular vector index is 113 token is tensor(0, device='cuda:0') sim is 0.2512510120868683\n",
      "SIM FOUND:head is 30 singular vector index is 106 token is tensor(4, device='cuda:0') sim is 0.2807904779911041\n",
      "SIM FOUND:head is 30 singular vector index is 106 token is tensor(4, device='cuda:0') sim is 0.2807904779911041\n",
      "SIM FOUND:head is 30 singular vector index is 113 token is tensor(0, device='cuda:0') sim is 0.2512510120868683\n",
      "SIM FOUND:head is 31 singular vector index is 31 token is tensor(2, device='cuda:0') sim is 0.2764180898666382\n",
      "SIM FOUND:head is 31 singular vector index is 121 token is tensor(2, device='cuda:0') sim is -0.25052130222320557\n",
      "SIM FOUND:head is 31 singular vector index is 118 token is tensor(1, device='cuda:0') sim is -0.2515212297439575\n",
      "SIM FOUND:head is 31 singular vector index is 5 token is tensor(8, device='cuda:0') sim is 0.2564130127429962\n",
      "SIM FOUND:head is 31 singular vector index is 121 token is tensor(8, device='cuda:0') sim is 0.2548947036266327\n",
      "SIM FOUND:head is 31 singular vector index is 31 token is tensor(2, device='cuda:0') sim is 0.2764180898666382\n",
      "SIM FOUND:head is 31 singular vector index is 121 token is tensor(2, device='cuda:0') sim is -0.25052130222320557\n",
      "SIM FOUND:head is 31 singular vector index is 31 token is tensor(2, device='cuda:0') sim is 0.2764180898666382\n",
      "SIM FOUND:head is 31 singular vector index is 121 token is tensor(2, device='cuda:0') sim is -0.25052130222320557\n",
      "SIM FOUND:head is 31 singular vector index is 118 token is tensor(1, device='cuda:0') sim is -0.2515212297439575\n",
      "{116: 3, 113: 8, 92: 11, 111: 3, 80: 4, 32: 1, 25: 8, 118: 15, 14: 9, 119: 18, 83: 11, 16: 8, 57: 13, 69: 9, 103: 15, 38: 6, 64: 6, 110: 9, 22: 7, 42: 2, 21: 2, 61: 2, 108: 5, 124: 10, 31: 8, 41: 1, 68: 6, 36: 4, 11: 2, 29: 6, 10: 1, 12: 8, 90: 4, 97: 5, 123: 4, 86: 4, 56: 6, 39: 5, 13: 5, 78: 4, 79: 3, 122: 8, 81: 8, 54: 2, 82: 12, 15: 5, 9: 1, 112: 6, 121: 12, 43: 1, 98: 1, 70: 1, 106: 7, 45: 3, 75: 3, 74: 1, 51: 1, 95: 2, 24: 3, 52: 4, 109: 6, 93: 3, 40: 9, 60: 4, 27: 3, 28: 5, 67: 3, 102: 3, 127: 5, 50: 2, 65: 7, 94: 5, 114: 2, 71: 3, 105: 5, 76: 1, 104: 3, 8: 1, 58: 2, 96: 1, 101: 4, 117: 5, 88: 2, 85: 3, 44: 6, 120: 4, 30: 2, 66: 2, 77: 1, 37: 1, 47: 2, 126: 1, 20: 1, 5: 1}\n",
      "(19, 128)\n"
     ]
    }
   ],
   "source": [
    "one_digit_additions_tokens = torch.tensor(tokenize_strings([\n",
    "    generate_digit_datapoint(123456, 789, n_digits, reversed_output, n_equals)\n",
    "          \n",
    "])).long().cuda()\n",
    "\n",
    "#print(one_digit_additions_tokens)\n",
    "\n",
    "tokens = torch.tensor(one_digit_additions_tokens).long().cuda()\n",
    "\n",
    "sims_head = svd_trace(model,tokens, N_singular_vectors = 128, sim_threshold = 0.25)\n",
    "#get the non-zero indices of sims_head\n",
    "#indices = np.nonzero(sims_head)\n",
    "\n",
    "print(sims_head.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
