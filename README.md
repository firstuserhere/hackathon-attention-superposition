# hackathon-attention-superposition


For this hackathon, we did 4 projects all about superposition. In particular, we:

- examining attention head superposition in a toy model of addition
- search for evidence of neuron superposition in TinyStories
- implement Neuroscope for TinyStories
- Train toy models for nonlinear computation in superposition, and reimplement their toy model results

![alt image](heatmap_animation.gif)
![alt image](heatmap_multi_animation.gif)
