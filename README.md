# hackathon-attention-superposition


For this hackathon, we did 4 projects all about superposition. In particular, we:

- examining attention head superposition in a toy model of addition ([Code](https://github.com/firstuserhere/hackathon-attention-superposition/blob/main/Integer_Addition_Transformer_Attention_Head_Superposition.ipynb))
- search for evidence of neuron superposition in TinyStories ([Code](https://github.com/firstuserhere/hackathon-attention-superposition/blob/main/Investigating_Superposition_in_Tinystories.ipynb)
- implement Neuroscope for TinyStories ([Code](https://github.com/joshuadavid/neuroscope))
- Train toy models for nonlinear computation in superposition, and reimplement their toy model results ([Code](https://github.com/firstuserhere/hackathon-attention-superposition/blob/main/Hackathon_Computation_in_superpositon_.ipynb))

![alt image](heatmap_animation.gif)
![alt image](heatmap_multi_animation.gif)
